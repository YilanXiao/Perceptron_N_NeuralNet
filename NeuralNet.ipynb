{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Neural_Net_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "g3serHLbBSZU",
        "_jUsQJHPhThp",
        "HTCdoezLDlDd",
        "0lFhwlWo_NOe",
        "xrx5vuBF6aR7",
        "Nhx1d1om5m9W",
        "mJ7IHK_86EKD",
        "0Hx2woZp7dyT",
        "7rGhDFtU7dyh",
        "THz9fjX18IHR",
        "rF_ZdkgV8IHd",
        "oY7OBuif9Vox",
        "mxBzgl2O9Vo2",
        "CIsxz8_A9nWM",
        "sa-HAQsV9nWd",
        "jqYZBphU95j9",
        "Da9zulP895kL",
        "0g7GzosP-VLy",
        "5Ormr0PI-VL2",
        "FK2gA9BX-ozR",
        "YdT89HB_-ozT",
        "QT0X-jjV_Ic-",
        "C0nWqZCE4AgH",
        "P9NO5tbgEhqx",
        "haMJMSSZ0dwn",
        "V7OOYsGjpyqF"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqu5kl3c4InZ"
      },
      "source": [
        "Implemented in Google Colab \n",
        "\n",
        "@Yilan Xiao"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3serHLbBSZU"
      },
      "source": [
        "#Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jUsQJHPhThp"
      },
      "source": [
        "##Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ujSUdIKn8v5",
        "outputId": "b53ac462-d18d-41cb-b2ec-743032bdaf00"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRSH0RjGqRlp"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from os import system, name\n",
        "from time import sleep\n",
        "from numpy import * "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTCdoezLDlDd"
      },
      "source": [
        "##Initialisation, Training and testing implementation for sigmoid and rectifier activation function (Q2.2, Q2.3, Q2.4, Q2.5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp8BBAbGZxth"
      },
      "source": [
        "\n",
        "class ANN:\n",
        "\n",
        "    #==========================================#\n",
        "    # The init method is called when an object #\n",
        "    # is created. It can be used to initialize #\n",
        "    # the attributes of the class.             #\n",
        "    #==========================================#\n",
        "    def __init__(self, input_layer_size, learning_rate, max_iterations, train_size, test_size, no_hidden_layers = 1, hidden_layer_size = 28, output_layer_size = 10):\n",
        "\n",
        "        self.input_layer_size = input_layer_size #input_layer_size: 1* no_inputs\n",
        "\n",
        "        self.learning_rate = learning_rate #start with 0.1\n",
        "        self.max_iterations = max_iterations #start with 20\n",
        "\n",
        "        self.no_hidden_layers = no_hidden_layers #1-10, but start with 1\n",
        "\n",
        "        self.hidden_layer_size = hidden_layer_size #start with 28, and let's keep them the same size for now\n",
        "\n",
        "        self.output_layer_size = output_layer_size\n",
        "        \n",
        "        #no_hidden_layer n = 1\n",
        "        #self.win = np.ones((hidden_layer_size,input_layer_size))/(hidden_layer_size * input_layer_size) #28*784 #W1 for layer one\n",
        "        self.win = np.random.randn(hidden_layer_size,input_layer_size) * np.sqrt(1/input_layer_size) #np.ones((hidden_layer_size,input_layer_size))/(hidden_layer_size * input_layer_size) #28*784 #W1 for layer one\n",
        "        self.bin = np.zeros((hidden_layer_size,1)) #28*1 #B1 for layer one                                           \n",
        "        \n",
        "        #self.w2 = np.ones((hidden_layer_size, hidden_layer_size))/(hidden_layer_size * hidden_layer_size) #28*28\n",
        "        self.w2 = np.random.randn(hidden_layer_size,hidden_layer_size) * np.sqrt(1/hidden_layer_size) #np.ones((hidden_layer_size, hidden_layer_size))/(hidden_layer_size * hidden_layer_size) #28*28\n",
        "        self.b2 = np.zeros((hidden_layer_size,1)) #28*1\n",
        "\n",
        "        #self.w3 = np.ones((hidden_layer_size, hidden_layer_size))/(hidden_layer_size * hidden_layer_size) #28*28\n",
        "        self.w3 = np.random.randn(hidden_layer_size,hidden_layer_size) * np.sqrt(1/hidden_layer_size)\n",
        "        self.b3 = np.zeros((hidden_layer_size,1)) #28*1\n",
        "\n",
        "        #self.w4 = np.ones((hidden_layer_size, hidden_layer_size))/(hidden_layer_size * hidden_layer_size) #28*28\n",
        "        self.w4 = np.random.randn(hidden_layer_size,hidden_layer_size) * np.sqrt(1/hidden_layer_size)\n",
        "        self.b4 = np.zeros((hidden_layer_size,1)) #28*1\n",
        "\n",
        "        #self.w5 = np.ones((hidden_layer_size, hidden_layer_size))/(hidden_layer_size * hidden_layer_size) #28*28\n",
        "        self.w5 = np.random.randn(hidden_layer_size,hidden_layer_size) * np.sqrt(1/hidden_layer_size)\n",
        "        self.b5 = np.zeros((hidden_layer_size,1)) #28*1\n",
        "\n",
        "        #self.w6 = np.ones((hidden_layer_size, hidden_layer_size))/(hidden_layer_size * hidden_layer_size) #28*28\n",
        "        self.w6 = np.random.randn(hidden_layer_size,hidden_layer_size) * np.sqrt(1/hidden_layer_size)        \n",
        "        self.b6 = np.zeros((hidden_layer_size,1)) #28*1\n",
        "\n",
        "        #self.w7 = np.ones((hidden_layer_size, hidden_layer_size))/(hidden_layer_size * hidden_layer_size) #28*28\n",
        "        self.w7 = np.random.randn(hidden_layer_size,hidden_layer_size) * np.sqrt(1/hidden_layer_size)        \n",
        "        self.b7 = np.zeros((hidden_layer_size,1)) #28*1\n",
        "\n",
        "        #self.w8 = np.ones((hidden_layer_size, hidden_layer_size))/(hidden_layer_size * hidden_layer_size) #28*28\n",
        "        self.w8 = np.random.randn(hidden_layer_size,hidden_layer_size) * np.sqrt(1/hidden_layer_size)        \n",
        "        self.b8 = np.zeros((hidden_layer_size,1)) #28*1\n",
        "\n",
        "        #self.w9 = np.ones((hidden_layer_size, hidden_layer_size))/(hidden_layer_size * hidden_layer_size) #28*28\n",
        "        self.w9 = np.random.randn(hidden_layer_size,hidden_layer_size) * np.sqrt(1/hidden_layer_size)        \n",
        "        self.b9 = np.zeros((hidden_layer_size,1)) #28*1\n",
        "\n",
        "        #self.w10 = np.ones((hidden_layer_size, hidden_layer_size))/(hidden_layer_size * hidden_layer_size) #28*28\n",
        "        self.w10 = np.random.randn(hidden_layer_size,hidden_layer_size) * np.sqrt(1/hidden_layer_size)        \n",
        "        self.b10 = np.zeros((hidden_layer_size,1)) #28*1\n",
        "\n",
        "        #self.wout = np.ones((output_layer_size,hidden_layer_size))/(output_layer_size * hidden_layer_size) #10*28 #W2 for layer two\n",
        "        self.wout = np.random.randn(output_layer_size,hidden_layer_size) * np.sqrt(1/hidden_layer_size)        \n",
        "        self.bout = np.zeros((output_layer_size,1)) #10*1 #B2 for layer two\n",
        "\n",
        "        self.train_size = train_size #60000\n",
        "        self.test_size = test_size #10000\n",
        "\n",
        "    #===========================================#\n",
        "    # Performs the sigmoid activation function. #\n",
        "    # Expects an array of values of             #\n",
        "    # shape (1,N) where N is the number         #\n",
        "    # of nodes in the layer.                    #\n",
        "    #===========================================#\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1+np.exp(-z))\n",
        "\n",
        "    #===========================================#\n",
        "    # Performs the relu activation function.    #\n",
        "    # Expects an array of values of             #\n",
        "    # shape (1,N) where N is the number         #\n",
        "    # of nodes in the layer.                    #\n",
        "    #===========================================#\n",
        "    def relu(self, z):\n",
        "        for i in range(len(z)):\n",
        "          if z[i] <= 0:\n",
        "            z[i] = 0\n",
        "        return z\n",
        "\n",
        "    #===============================#\n",
        "    # Trains the net using labelled #\n",
        "    # training data.                #\n",
        "    #===============================#\n",
        "    def sigmoid(self, z):\n",
        "        z = np.asarray(z)\n",
        "        z = 1/(1+np.exp(-z))\n",
        "        return z\n",
        "\n",
        "    def sigmoid_prime(self, z):\n",
        "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
        "\n",
        "    def relu_prime(self, z):\n",
        "        for i in range(len(z)):\n",
        "          if z[i] <= 0:\n",
        "            z[i] = 0\n",
        "          else:\n",
        "            z[i] = 1\n",
        "        return z\n",
        "        \n",
        "    #======================================#\n",
        "    # Trains the multi-class perceptron    #\n",
        "    # using labelled training data.        #\n",
        "    #======================================#\n",
        "    def train_multi_class_1(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "          #shuffle training data\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "          #weight_update = self.weight_update\n",
        "          #bias_update = self.bias_update \n",
        "          \n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.sigmoid(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "            \n",
        "            z2 = np.dot(self.wout, a1) + self.bout # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "            output = self.sigmoid(z2) #1*10\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe2 = output - label #1*10 #syntax needs testing\n",
        "            ae2 = np.multiply(oe2, self.sigmoid_prime(z2)) #element wise multiplication\n",
        "\n",
        "            g2w = np.dot(ae2,a1.T) #output gradient# ae2 (10,1)# a1 (28,1)# output_gradient shape same as w2(10,28)          \n",
        "            g2b = ae2 #(10,1) #output_bias_gradient = self.sigmoid_prime(z2) * output_error \n",
        "            \n",
        "            self.wout = self.wout - self.learning_rate*g2w\n",
        "            self.bout = self.bout - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.wout.T, ae2)# w2 (10,28) # ae2 (10,1) # oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.sigmoid_prime(z1))# self.sigmoid_prime(z1) (28,1)#  ae1 (28,1)          \n",
        "            g1w = np.dot(ae1, a0.T)#a0 (784,1)#g1 (28,784)\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w #(28, 784) \n",
        "            self.bin = self.bin - self.learning_rate*g1b                      \n",
        "\n",
        "    def train_multi_class_1_relu(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "          #shuffle training data\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            #print(\"z1\",z1)\n",
        "            a1 = self.relu(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "            #print(\"a1\",a1)\n",
        "\n",
        "            z2 = np.dot(self.wout, a1) + self.bout # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "            #print(\"z2\", z2)\n",
        "            output = self.relu(z2) #1*10\n",
        "            #print(\"output\", output)\n",
        "\n",
        "            oe2 = output - label\n",
        "            ae2 = np.multiply(oe2, self.relu_prime(z2)) #element wise multiplication # output error shape (10,1), sigmoid_prime shape (10,1)# ae2 shape (10,1)\n",
        "\n",
        "            g2w = np.dot(ae2,a1.T) #output gradient # ae2 (10,1) # a1 (28,1) # output_gradient shape same as w2(10,28)\n",
        "\n",
        "            g2b = ae2 #(10,1) \n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g2w\n",
        "            self.bout = self.bout - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.wout.T, ae2) # w2 (10,28) # ae2 (10,1) # oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.relu_prime(z1))\n",
        "       \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "            \n",
        "  \n",
        "    def train_multi_class_2(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.sigmoid(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.sigmoid(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.wout, a2) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.sigmoid(z3) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe3 = output - label #1*10 #syntax needs testing\n",
        "            ae3 = np.multiply(oe3, self.sigmoid_prime(z3)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g3w = np.dot(ae3,a2.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g3b = ae3\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g3w\n",
        "            self.bout = self.bout - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.wout.T, ae3)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.sigmoid_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.sigmoid_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        " \n",
        "    def train_multi_class_2_relu(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.relu(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.relu(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.wout, a2) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.relu(z3) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe3 = output - label #1*10 #syntax needs testing\n",
        "            ae3 = np.multiply(oe3, self.relu_prime(z3)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g3w = np.dot(ae3,a2.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g3b = ae3\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g3w\n",
        "            self.bout = self.bout - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.wout.T, ae3)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.relu_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.relu_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "                                                                \n",
        "#To add in\n",
        "    def train_multi_class_3(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.sigmoid(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.sigmoid(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.wout, a3) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.sigmoid(z4) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe4 = output - label #1*10 #syntax needs testing\n",
        "            ae4 = np.multiply(oe4, self.sigmoid_prime(z4)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g4w = np.dot(ae4,a3.T) #output gradient\n",
        "            # output_gradient shape same as (10,28) ae4 (10,1) a3 (28,1)\n",
        "            g4b = ae4\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g4w\n",
        "            self.bout = self.bout - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.wout.T, ae4)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.sigmoid_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # wout (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.sigmoid_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.sigmoid_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "                       \n",
        "#To add in\n",
        "    def train_multi_class_3_relu(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.relu(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.relu(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.relu(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.wout, a3) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.relu(z4) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe4 = output - label #1*10 #syntax needs testing\n",
        "            ae4 = np.multiply(oe4, self.relu_prime(z4)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g4w = np.dot(ae4,a3.T) #output gradient\n",
        "            # output_gradient shape same as (10,28) ae4 (10,1) a3 (28,1)\n",
        "            g4b = ae4\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g4w\n",
        "            self.bout = self.bout - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.wout.T, ae4)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.relu_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # wout (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.relu_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.relu_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "                       \n",
        "#To add in\n",
        "    def train_multi_class_4(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.sigmoid(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.sigmoid(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a4 = self.sigmoid(z4) #28,1\n",
        "\n",
        "            z5 = np.dot(self.wout, a4) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.sigmoid(z5) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe5 = output - label #1*10 #syntax needs testing\n",
        "            ae5 = np.multiply(oe5, self.sigmoid_prime(z5)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g5w = np.dot(ae5,a4.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g5b = ae5\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g5w\n",
        "            self.bout = self.bout - self.learning_rate*g5b\n",
        "\n",
        "            oe4 = np.dot(self.wout.T, ae5)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae4 = np.multiply(oe4,self.sigmoid_prime(z4))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g4w = np.dot(ae4, a3.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g4b = ae4\n",
        "\n",
        "            self.w4 = self.w4 - self.learning_rate*g4w\n",
        "            self.b4 = self.b4 - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.w4.T, ae4)\n",
        "            # w3 (28,28) ae4 (28,1) oe3 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.sigmoid_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # w3 (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.sigmoid_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.sigmoid_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "                       \n",
        "#To add in\n",
        "    def train_multi_class_4_relu(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.relu(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.relu(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.relu(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a4 = self.relu(z4) #28,1\n",
        "\n",
        "            z5 = np.dot(self.wout, a4) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.relu(z5) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe5 = output - label #1*10 #syntax needs testing\n",
        "            ae5 = np.multiply(oe5, self.relu_prime(z5)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g5w = np.dot(ae5,a4.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g5b = ae5\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g5w\n",
        "            self.bout = self.bout - self.learning_rate*g5b\n",
        "\n",
        "            oe4 = np.dot(self.wout.T, ae5)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae4 = np.multiply(oe4,self.relu_prime(z4))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g4w = np.dot(ae4, a3.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g4b = ae4\n",
        "\n",
        "            self.w4 = self.w4 - self.learning_rate*g4w\n",
        "            self.b4 = self.b4 - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.w4.T, ae4)\n",
        "            # w3 (28,28) ae4 (28,1) oe3 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.relu_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # w3 (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.relu_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.relu_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "\n",
        " #To add in\n",
        "    def train_multi_class_5(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.sigmoid(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.sigmoid(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a4 = self.sigmoid(z4) #28,1\n",
        "\n",
        "            z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a5 = self.sigmoid(z5) #28,1\n",
        "\n",
        "            z6 = np.dot(self.wout, a5) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.sigmoid(z6) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe6 = output - label #1*10 #syntax needs testing\n",
        "            ae6 = np.multiply(oe6, self.sigmoid_prime(z6)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g6w = np.dot(ae6,a5.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g6b = ae6\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g6w\n",
        "            self.bout = self.bout - self.learning_rate*g6b\n",
        "\n",
        "            oe5 = np.dot(self.wout.T, ae6)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae5 = np.multiply(oe5,self.sigmoid_prime(z5))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g5w = np.dot(ae5, a4.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g5b = ae5\n",
        "\n",
        "            self.w5 = self.w5 - self.learning_rate*g5w\n",
        "            self.b5 = self.b5 - self.learning_rate*g5b\n",
        "\n",
        "            oe4 = np.dot(self.w5.T, ae5)\n",
        "            # w5 (28,28) ae5 (28,1) oe4 (28,1)\n",
        "            ae4 = np.multiply(oe4,self.sigmoid_prime(z4))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g4w = np.dot(ae4, a3.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g4b = ae4\n",
        "\n",
        "            self.w4 = self.w4 - self.learning_rate*g4w\n",
        "            self.b4 = self.b4 - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.w4.T, ae4)\n",
        "            # w3 (28,28) ae4 (28,1) oe3 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.sigmoid_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # w3 (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.sigmoid_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.sigmoid_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "\n",
        "#To add in\n",
        "    def train_multi_class_5_relu(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.relu(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.relu(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.relu(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a4 = self.relu(z4) #28,1\n",
        "\n",
        "            z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a5 = self.relu(z5) #28,1\n",
        "\n",
        "            z6 = np.dot(self.wout, a5) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.relu(z6) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe6 = output - label #1*10 #syntax needs testing\n",
        "            ae6 = np.multiply(oe6, self.relu_prime(z6)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g6w = np.dot(ae6,a5.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g6b = ae6\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g6w\n",
        "            self.bout = self.bout - self.learning_rate*g6b\n",
        "\n",
        "            oe5 = np.dot(self.wout.T, ae6)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae5 = np.multiply(oe5,self.relu_prime(z5))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g5w = np.dot(ae5, a4.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g5b = ae5\n",
        "\n",
        "            self.w5 = self.w5 - self.learning_rate*g5w\n",
        "            self.b5 = self.b5 - self.learning_rate*g5b\n",
        "\n",
        "            oe4 = np.dot(self.w5.T, ae5)\n",
        "            # w5 (28,28) ae5 (28,1) oe4 (28,1)\n",
        "            ae4 = np.multiply(oe4,self.relu_prime(z4))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g4w = np.dot(ae4, a3.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g4b = ae4\n",
        "\n",
        "            self.w4 = self.w4 - self.learning_rate*g4w\n",
        "            self.b4 = self.b4 - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.w4.T, ae4)\n",
        "            # w3 (28,28) ae4 (28,1) oe3 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.relu_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # w3 (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.relu_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.relu_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "                                          \n",
        "#To add in\n",
        "    def train_multi_class_6(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.sigmoid(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.sigmoid(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a4 = self.sigmoid(z4) #28,1\n",
        "\n",
        "            z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a5 = self.sigmoid(z5) #28,1\n",
        "\n",
        "            z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a6 = self.sigmoid(z6) #28,1\n",
        "\n",
        "            z7 = np.dot(self.wout, a6) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.sigmoid(z7) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe7 = output - label #1*10 #syntax needs testing\n",
        "            ae7 = np.multiply(oe7, self.sigmoid_prime(z7)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g7w = np.dot(ae7,a6.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g7b = ae7\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g7w\n",
        "            self.bout = self.bout - self.learning_rate*g7b\n",
        "\n",
        "            oe6 = np.dot(self.wout.T, ae7)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae6 = np.multiply(oe6,self.sigmoid_prime(z6))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g6w = np.dot(ae6, a5.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g6b = ae6\n",
        "\n",
        "            self.w6 = self.w6 - self.learning_rate*g6w\n",
        "            self.b6 = self.b6 - self.learning_rate*g6b\n",
        "\n",
        "            oe5 = np.dot(self.w6.T, ae6)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae5 = np.multiply(oe5,self.sigmoid_prime(z5))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g5w = np.dot(ae5, a4.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g5b = ae5\n",
        "\n",
        "            self.w5 = self.w5 - self.learning_rate*g5w\n",
        "            self.b5 = self.b5 - self.learning_rate*g5b\n",
        "\n",
        "            oe4 = np.dot(self.w5.T, ae5)\n",
        "            # w5 (28,28) ae5 (28,1) oe4 (28,1)\n",
        "            ae4 = np.multiply(oe4,self.sigmoid_prime(z4))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g4w = np.dot(ae4, a3.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g4b = ae4\n",
        "\n",
        "            self.w4 = self.w4 - self.learning_rate*g4w\n",
        "            self.b4 = self.b4 - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.w4.T, ae4)\n",
        "            # w3 (28,28) ae4 (28,1) oe3 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.sigmoid_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # w3 (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.sigmoid_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.sigmoid_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "                       \n",
        "#To add in\n",
        "    def train_multi_class_6_relu(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.relu(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.relu(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.relu(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a4 = self.relu(z4) #28,1\n",
        "\n",
        "            z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a5 = self.relu(z5) #28,1\n",
        "\n",
        "            z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a6 = self.relu(z6) #28,1\n",
        "\n",
        "            z7 = np.dot(self.wout, a6) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.relu(z7) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe7 = output - label #1*10 #syntax needs testing\n",
        "            ae7 = np.multiply(oe7, self.relu_prime(z7)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g7w = np.dot(ae7,a6.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g7b = ae7\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g7w\n",
        "            self.bout = self.bout - self.learning_rate*g7b\n",
        "\n",
        "            oe6 = np.dot(self.wout.T, ae7)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae6 = np.multiply(oe6,self.relu_prime(z6))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g6w = np.dot(ae6, a5.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g6b = ae6\n",
        "\n",
        "            self.w6 = self.w6 - self.learning_rate*g6w\n",
        "            self.b6 = self.b6 - self.learning_rate*g6b\n",
        "\n",
        "            oe5 = np.dot(self.w6.T, ae6)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae5 = np.multiply(oe5,self.relu_prime(z5))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g5w = np.dot(ae5, a4.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g5b = ae5\n",
        "\n",
        "            self.w5 = self.w5 - self.learning_rate*g5w\n",
        "            self.b5 = self.b5 - self.learning_rate*g5b\n",
        "\n",
        "            oe4 = np.dot(self.w5.T, ae5)\n",
        "            # w5 (28,28) ae5 (28,1) oe4 (28,1)\n",
        "            ae4 = np.multiply(oe4,self.relu_prime(z4))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g4w = np.dot(ae4, a3.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g4b = ae4\n",
        "\n",
        "            self.w4 = self.w4 - self.learning_rate*g4w\n",
        "            self.b4 = self.b4 - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.w4.T, ae4)\n",
        "            # w3 (28,28) ae4 (28,1) oe3 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.relu_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # w3 (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.relu_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.relu_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "\n",
        "#To add in\n",
        "    def train_multi_class_7(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.sigmoid(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.sigmoid(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a4 = self.sigmoid(z4) #28,1\n",
        "\n",
        "            z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a5 = self.sigmoid(z5) #28,1\n",
        "\n",
        "            z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a6 = self.sigmoid(z6) #28,1\n",
        "\n",
        "            z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a7 = self.sigmoid(z7) #28,1\n",
        "\n",
        "            z8 = np.dot(self.wout, a7) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.sigmoid(z8) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe8 = output - label #1*10 #syntax needs testing\n",
        "            ae8 = np.multiply(oe8, self.sigmoid_prime(z8)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g8w = np.dot(ae8,a7.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g8b = ae8\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g8w\n",
        "            self.bout = self.bout - self.learning_rate*g8b\n",
        "\n",
        "            oe7 = np.dot(self.wout.T, ae8)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae7 = np.multiply(oe7,self.sigmoid_prime(z7))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g7w = np.dot(ae7, a6.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g7b = ae7\n",
        "\n",
        "            self.w7 = self.w7 - self.learning_rate*g7w\n",
        "            self.b7 = self.b7 - self.learning_rate*g7b\n",
        "\n",
        "            oe6 = np.dot(self.w7.T, ae7)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae6 = np.multiply(oe6,self.sigmoid_prime(z6))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g6w = np.dot(ae6, a5.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g6b = ae6\n",
        "\n",
        "            self.w6 = self.w6 - self.learning_rate*g6w\n",
        "            self.b6 = self.b6 - self.learning_rate*g6b\n",
        "\n",
        "            oe5 = np.dot(self.w6.T, ae6)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae5 = np.multiply(oe5,self.sigmoid_prime(z5))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g5w = np.dot(ae5, a4.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g5b = ae5\n",
        "\n",
        "            self.w5 = self.w5 - self.learning_rate*g5w\n",
        "            self.b5 = self.b5 - self.learning_rate*g5b\n",
        "\n",
        "            oe4 = np.dot(self.w5.T, ae5)\n",
        "            # w5 (28,28) ae5 (28,1) oe4 (28,1)\n",
        "            ae4 = np.multiply(oe4,self.sigmoid_prime(z4))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g4w = np.dot(ae4, a3.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g4b = ae4\n",
        "\n",
        "            self.w4 = self.w4 - self.learning_rate*g4w\n",
        "            self.b4 = self.b4 - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.w4.T, ae4)\n",
        "            # w3 (28,28) ae4 (28,1) oe3 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.sigmoid_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # w3 (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.sigmoid_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.sigmoid_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "                       \n",
        "#To add in\n",
        "    def train_multi_class_7_relu(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.relu(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.relu(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.relu(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a4 = self.relu(z4) #28,1\n",
        "\n",
        "            z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a5 = self.relu(z5) #28,1\n",
        "\n",
        "            z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a6 = self.relu(z6) #28,1\n",
        "\n",
        "            z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a7 = self.relu(z7) #28,1\n",
        "\n",
        "            z8 = np.dot(self.wout, a7) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.relu(z8) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe8 = output - label #1*10 #syntax needs testing\n",
        "            ae8 = np.multiply(oe8, self.relu_prime(z8)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g8w = np.dot(ae8,a7.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g8b = ae8\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g8w\n",
        "            self.bout = self.bout - self.learning_rate*g8b\n",
        "\n",
        "            oe7 = np.dot(self.wout.T, ae8)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae7 = np.multiply(oe7,self.relu_prime(z7))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g7w = np.dot(ae7, a6.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g7b = ae7\n",
        "\n",
        "            self.w7 = self.w7 - self.learning_rate*g7w\n",
        "            self.b7 = self.b7 - self.learning_rate*g7b\n",
        "\n",
        "            oe6 = np.dot(self.w7.T, ae7)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae6 = np.multiply(oe6,self.relu_prime(z6))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g6w = np.dot(ae6, a5.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g6b = ae6\n",
        "\n",
        "            self.w6 = self.w6 - self.learning_rate*g6w\n",
        "            self.b6 = self.b6 - self.learning_rate*g6b\n",
        "\n",
        "            oe5 = np.dot(self.w6.T, ae6)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae5 = np.multiply(oe5,self.relu_prime(z5))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g5w = np.dot(ae5, a4.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g5b = ae5\n",
        "\n",
        "            self.w5 = self.w5 - self.learning_rate*g5w\n",
        "            self.b5 = self.b5 - self.learning_rate*g5b\n",
        "\n",
        "            oe4 = np.dot(self.w5.T, ae5)\n",
        "            # w5 (28,28) ae5 (28,1) oe4 (28,1)\n",
        "            ae4 = np.multiply(oe4,self.relu_prime(z4))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g4w = np.dot(ae4, a3.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g4b = ae4\n",
        "\n",
        "            self.w4 = self.w4 - self.learning_rate*g4w\n",
        "            self.b4 = self.b4 - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.w4.T, ae4)\n",
        "            # w3 (28,28) ae4 (28,1) oe3 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.relu_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # w3 (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.relu_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.relu_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "                       \n",
        " #To add in\n",
        "    def train_multi_class_8(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.sigmoid(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.sigmoid(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a4 = self.sigmoid(z4) #28,1\n",
        "\n",
        "            z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a5 = self.sigmoid(z5) #28,1\n",
        "\n",
        "            z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a6 = self.sigmoid(z6) #28,1\n",
        "\n",
        "            z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a7 = self.sigmoid(z7) #28,1\n",
        "\n",
        "            z8 = np.dot(self.w8, a7) + self.b8 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a8 = self.sigmoid(z8) #28,1\n",
        "\n",
        "            z9 = np.dot(self.wout, a8) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.sigmoid(z9) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe9 = output - label #1*10 #syntax needs testing\n",
        "            ae9 = np.multiply(oe9, self.sigmoid_prime(z9)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g9w = np.dot(ae9,a8.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g9b = ae9\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g9w\n",
        "            self.bout = self.bout - self.learning_rate*g9b\n",
        "\n",
        "            oe8 = np.dot(self.wout.T, ae9)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae8 = np.multiply(oe8,self.sigmoid_prime(z8))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g8w = np.dot(ae8, a7.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g8b = ae8\n",
        "\n",
        "            self.w8 = self.w8 - self.learning_rate*g8w\n",
        "            self.b8 = self.b8 - self.learning_rate*g8b\n",
        "\n",
        "            oe7 = np.dot(self.w8.T, ae8)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae7 = np.multiply(oe7,self.sigmoid_prime(z7))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g7w = np.dot(ae7, a6.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g7b = ae7\n",
        "\n",
        "            self.w7 = self.w7 - self.learning_rate*g7w\n",
        "            self.b7 = self.b7 - self.learning_rate*g7b\n",
        "\n",
        "            oe6 = np.dot(self.w7.T, ae7)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae6 = np.multiply(oe6,self.sigmoid_prime(z6))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g6w = np.dot(ae6, a5.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g6b = ae6\n",
        "\n",
        "            self.w6 = self.w6 - self.learning_rate*g6w\n",
        "            self.b6 = self.b6 - self.learning_rate*g6b\n",
        "\n",
        "            oe5 = np.dot(self.w6.T, ae6)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae5 = np.multiply(oe5,self.sigmoid_prime(z5))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g5w = np.dot(ae5, a4.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g5b = ae5\n",
        "\n",
        "            self.w5 = self.w5 - self.learning_rate*g5w\n",
        "            self.b5 = self.b5 - self.learning_rate*g5b\n",
        "\n",
        "            oe4 = np.dot(self.w5.T, ae5)\n",
        "            # w5 (28,28) ae5 (28,1) oe4 (28,1)\n",
        "            ae4 = np.multiply(oe4,self.sigmoid_prime(z4))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g4w = np.dot(ae4, a3.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g4b = ae4\n",
        "\n",
        "            self.w4 = self.w4 - self.learning_rate*g4w\n",
        "            self.b4 = self.b4 - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.w4.T, ae4)\n",
        "            # w3 (28,28) ae4 (28,1) oe3 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.sigmoid_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # w3 (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.sigmoid_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.sigmoid_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "\n",
        "\n",
        "#To add in\n",
        "    def train_multi_class_8_relu(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.relu(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.relu(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.relu(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a4 = self.relu(z4) #28,1\n",
        "\n",
        "            z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a5 = self.relu(z5) #28,1\n",
        "\n",
        "            z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a6 = self.relu(z6) #28,1\n",
        "\n",
        "            z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a7 = self.relu(z7) #28,1\n",
        "\n",
        "            z8 = np.dot(self.w8, a7) + self.b8 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a8 = self.relu(z8) #28,1\n",
        "\n",
        "            z9 = np.dot(self.wout, a8) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.relu(z9) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe9 = output - label #1*10 #syntax needs testing\n",
        "            ae9 = np.multiply(oe9, self.relu_prime(z9)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g9w = np.dot(ae9,a8.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g9b = ae9\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g9w\n",
        "            self.bout = self.bout - self.learning_rate*g9b\n",
        "\n",
        "            oe8 = np.dot(self.wout.T, ae9)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae8 = np.multiply(oe8,self.relu_prime(z8))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g8w = np.dot(ae8, a7.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g8b = ae8\n",
        "\n",
        "            self.w8 = self.w8 - self.learning_rate*g8w\n",
        "            self.b8 = self.b8 - self.learning_rate*g8b\n",
        "\n",
        "            oe7 = np.dot(self.w8.T, ae8)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae7 = np.multiply(oe7,self.relu_prime(z7))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g7w = np.dot(ae7, a6.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g7b = ae7\n",
        "\n",
        "            self.w7 = self.w7 - self.learning_rate*g7w\n",
        "            self.b7 = self.b7 - self.learning_rate*g7b\n",
        "\n",
        "            oe6 = np.dot(self.w7.T, ae7)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae6 = np.multiply(oe6,self.relu_prime(z6))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g6w = np.dot(ae6, a5.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g6b = ae6\n",
        "\n",
        "            self.w6 = self.w6 - self.learning_rate*g6w\n",
        "            self.b6 = self.b6 - self.learning_rate*g6b\n",
        "\n",
        "            oe5 = np.dot(self.w6.T, ae6)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae5 = np.multiply(oe5,self.relu_prime(z5))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g5w = np.dot(ae5, a4.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g5b = ae5\n",
        "\n",
        "            self.w5 = self.w5 - self.learning_rate*g5w\n",
        "            self.b5 = self.b5 - self.learning_rate*g5b\n",
        "\n",
        "            oe4 = np.dot(self.w5.T, ae5)\n",
        "            # w5 (28,28) ae5 (28,1) oe4 (28,1)\n",
        "            ae4 = np.multiply(oe4,self.relu_prime(z4))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g4w = np.dot(ae4, a3.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g4b = ae4\n",
        "\n",
        "            self.w4 = self.w4 - self.learning_rate*g4w\n",
        "            self.b4 = self.b4 - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.w4.T, ae4)\n",
        "            # w3 (28,28) ae4 (28,1) oe3 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.relu_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # w3 (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.relu_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.relu_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "                       \n",
        " #To add in\n",
        "    def train_multi_class_9(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.sigmoid(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.sigmoid(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a4 = self.sigmoid(z4) #28,1\n",
        "\n",
        "            z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a5 = self.sigmoid(z5) #28,1\n",
        "\n",
        "            z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a6 = self.sigmoid(z6) #28,1\n",
        "\n",
        "            z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a7 = self.sigmoid(z7) #28,1\n",
        "\n",
        "            z8 = np.dot(self.w8, a7) + self.b8 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a8 = self.sigmoid(z8) #28,1\n",
        "\n",
        "            z9 = np.dot(self.w9, a8) + self.b9 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a9 = self.sigmoid(z9) #28,1\n",
        "\n",
        "            z10 = np.dot(self.wout, a9) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.sigmoid(z10) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe10 = output - label #1*10 #syntax needs testing\n",
        "            ae10 = np.multiply(oe10, self.sigmoid_prime(z10)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g10w = np.dot(ae10,a9.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g10b = ae10\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g10w\n",
        "            self.bout = self.bout - self.learning_rate*g10b\n",
        "\n",
        "            oe9 = np.dot(self.wout.T, ae10)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae9 = np.multiply(oe9,self.sigmoid_prime(z9))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g9w = np.dot(ae9, a8.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g9b = ae9\n",
        "\n",
        "            self.w9 = self.w9 - self.learning_rate*g9w\n",
        "            self.b9 = self.b9 - self.learning_rate*g9b\n",
        "\n",
        "            oe8 = np.dot(self.w9.T, ae9)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae8 = np.multiply(oe8,self.sigmoid_prime(z8))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g8w = np.dot(ae8, a7.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g8b = ae8\n",
        "\n",
        "            self.w8 = self.w8 - self.learning_rate*g8w\n",
        "            self.b8 = self.b8 - self.learning_rate*g8b\n",
        "\n",
        "            oe7 = np.dot(self.w8.T, ae8)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae7 = np.multiply(oe7,self.sigmoid_prime(z7))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g7w = np.dot(ae7, a6.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g7b = ae7\n",
        "\n",
        "            self.w7 = self.w7 - self.learning_rate*g7w\n",
        "            self.b7 = self.b7 - self.learning_rate*g7b\n",
        "\n",
        "            oe6 = np.dot(self.w7.T, ae7)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae6 = np.multiply(oe6,self.sigmoid_prime(z6))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g6w = np.dot(ae6, a5.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g6b = ae6\n",
        "\n",
        "            self.w6 = self.w6 - self.learning_rate*g6w\n",
        "            self.b6 = self.b6 - self.learning_rate*g6b\n",
        "\n",
        "            oe5 = np.dot(self.w6.T, ae6)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae5 = np.multiply(oe5,self.sigmoid_prime(z5))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g5w = np.dot(ae5, a4.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g5b = ae5\n",
        "\n",
        "            self.w5 = self.w5 - self.learning_rate*g5w\n",
        "            self.b5 = self.b5 - self.learning_rate*g5b\n",
        "\n",
        "            oe4 = np.dot(self.w5.T, ae5)\n",
        "            # w5 (28,28) ae5 (28,1) oe4 (28,1)\n",
        "            ae4 = np.multiply(oe4,self.sigmoid_prime(z4))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g4w = np.dot(ae4, a3.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g4b = ae4\n",
        "\n",
        "            self.w4 = self.w4 - self.learning_rate*g4w\n",
        "            self.b4 = self.b4 - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.w4.T, ae4)\n",
        "            # w3 (28,28) ae4 (28,1) oe3 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.sigmoid_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # w3 (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.sigmoid_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.sigmoid_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "\n",
        " #To add in\n",
        "    def train_multi_class_9_relu(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.relu(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.relu(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.relu(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a4 = self.relu(z4) #28,1\n",
        "\n",
        "            z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a5 = self.relu(z5) #28,1\n",
        "\n",
        "            z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a6 = self.relu(z6) #28,1\n",
        "\n",
        "            z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a7 = self.relu(z7) #28,1\n",
        "\n",
        "            z8 = np.dot(self.w8, a7) + self.b8 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a8 = self.relu(z8) #28,1\n",
        "\n",
        "            z9 = np.dot(self.w9, a8) + self.b9 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a9 = self.relu(z9) #28,1\n",
        "\n",
        "            z10 = np.dot(self.wout, a9) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.relu(z10) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe10 = output - label #1*10 #syntax needs testing\n",
        "            ae10 = np.multiply(oe10, self.relu_prime(z10)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g10w = np.dot(ae10,a9.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g10b = ae10\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g10w\n",
        "            self.bout = self.bout - self.learning_rate*g10b\n",
        "\n",
        "            oe9 = np.dot(self.wout.T, ae10)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae9 = np.multiply(oe9,self.relu_prime(z9))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g9w = np.dot(ae9, a8.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g9b = ae9\n",
        "\n",
        "            self.w9 = self.w9 - self.learning_rate*g9w\n",
        "            self.b9 = self.b9 - self.learning_rate*g9b\n",
        "\n",
        "            oe8 = np.dot(self.w9.T, ae9)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae8 = np.multiply(oe8,self.relu_prime(z8))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g8w = np.dot(ae8, a7.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g8b = ae8\n",
        "\n",
        "            self.w8 = self.w8 - self.learning_rate*g8w\n",
        "            self.b8 = self.b8 - self.learning_rate*g8b\n",
        "\n",
        "            oe7 = np.dot(self.w8.T, ae8)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae7 = np.multiply(oe7,self.relu_prime(z7))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g7w = np.dot(ae7, a6.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g7b = ae7\n",
        "\n",
        "            self.w7 = self.w7 - self.learning_rate*g7w\n",
        "            self.b7 = self.b7 - self.learning_rate*g7b\n",
        "\n",
        "            oe6 = np.dot(self.w7.T, ae7)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae6 = np.multiply(oe6,self.relu_prime(z6))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g6w = np.dot(ae6, a5.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g6b = ae6\n",
        "\n",
        "            self.w6 = self.w6 - self.learning_rate*g6w\n",
        "            self.b6 = self.b6 - self.learning_rate*g6b\n",
        "\n",
        "            oe5 = np.dot(self.w6.T, ae6)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae5 = np.multiply(oe5,self.relu_prime(z5))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g5w = np.dot(ae5, a4.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g5b = ae5\n",
        "\n",
        "            self.w5 = self.w5 - self.learning_rate*g5w\n",
        "            self.b5 = self.b5 - self.learning_rate*g5b\n",
        "\n",
        "            oe4 = np.dot(self.w5.T, ae5)\n",
        "            # w5 (28,28) ae5 (28,1) oe4 (28,1)\n",
        "            ae4 = np.multiply(oe4,self.relu_prime(z4))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g4w = np.dot(ae4, a3.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g4b = ae4\n",
        "\n",
        "            self.w4 = self.w4 - self.learning_rate*g4w\n",
        "            self.b4 = self.b4 - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.w4.T, ae4)\n",
        "            # w3 (28,28) ae4 (28,1) oe3 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.relu_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # w3 (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.relu_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.relu_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "\n",
        "#To add in\n",
        "    def train_multi_class_10(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.sigmoid(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.sigmoid(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a4 = self.sigmoid(z4) #28,1\n",
        "\n",
        "            z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a5 = self.sigmoid(z5) #28,1\n",
        "\n",
        "            z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a6 = self.sigmoid(z6) #28,1\n",
        "\n",
        "            z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a7 = self.sigmoid(z7) #28,1\n",
        "\n",
        "            z8 = np.dot(self.w8, a7) + self.b8 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a8 = self.sigmoid(z8) #28,1\n",
        "\n",
        "            z9 = np.dot(self.w9, a8) + self.b9 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a9 = self.sigmoid(z9) #28,1\n",
        "\n",
        "            z10 = np.dot(self.w10, a9) + self.b10 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a10 = self.sigmoid(z10) #28,1\n",
        "\n",
        "            z11 = np.dot(self.wout, a10) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.sigmoid(z11) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe11 = output - label #1*10 #syntax needs testing\n",
        "            ae11 = np.multiply(oe11, self.sigmoid_prime(z11)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g11w = np.dot(ae11,a10.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g11b = ae11\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g11w\n",
        "            self.bout = self.bout - self.learning_rate*g11b\n",
        "\n",
        "            oe10 = np.dot(self.wout.T, ae11)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae10 = np.multiply(oe10,self.sigmoid_prime(z10))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g10w = np.dot(ae10, a9.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g10b = ae10\n",
        "\n",
        "            oe9 = np.dot(self.w10.T, ae10)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae9 = np.multiply(oe9,self.sigmoid_prime(z9))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g9w = np.dot(ae9, a8.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g9b = ae9\n",
        "\n",
        "            self.w9 = self.w9 - self.learning_rate*g9w\n",
        "            self.b9 = self.b9 - self.learning_rate*g9b\n",
        "\n",
        "            oe8 = np.dot(self.w9.T, ae9)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae8 = np.multiply(oe8,self.sigmoid_prime(z8))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g8w = np.dot(ae8, a7.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g8b = ae8\n",
        "\n",
        "            self.w8 = self.w8 - self.learning_rate*g8w\n",
        "            self.b8 = self.b8 - self.learning_rate*g8b\n",
        "\n",
        "            oe7 = np.dot(self.w8.T, ae8)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae7 = np.multiply(oe7,self.sigmoid_prime(z7))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g7w = np.dot(ae7, a6.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g7b = ae7\n",
        "\n",
        "            self.w7 = self.w7 - self.learning_rate*g7w\n",
        "            self.b7 = self.b7 - self.learning_rate*g7b\n",
        "\n",
        "            oe6 = np.dot(self.w7.T, ae7)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae6 = np.multiply(oe6,self.sigmoid_prime(z6))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g6w = np.dot(ae6, a5.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g6b = ae6\n",
        "\n",
        "            self.w6 = self.w6 - self.learning_rate*g6w\n",
        "            self.b6 = self.b6 - self.learning_rate*g6b\n",
        "\n",
        "            oe5 = np.dot(self.w6.T, ae6)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae5 = np.multiply(oe5,self.sigmoid_prime(z5))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g5w = np.dot(ae5, a4.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g5b = ae5\n",
        "\n",
        "            self.w5 = self.w5 - self.learning_rate*g5w\n",
        "            self.b5 = self.b5 - self.learning_rate*g5b\n",
        "\n",
        "            oe4 = np.dot(self.w5.T, ae5)\n",
        "            # w5 (28,28) ae5 (28,1) oe4 (28,1)\n",
        "            ae4 = np.multiply(oe4,self.sigmoid_prime(z4))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g4w = np.dot(ae4, a3.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g4b = ae4\n",
        "\n",
        "            self.w4 = self.w4 - self.learning_rate*g4w\n",
        "            self.b4 = self.b4 - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.w4.T, ae4)\n",
        "            # w3 (28,28) ae4 (28,1) oe3 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.sigmoid_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # w3 (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.sigmoid_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.sigmoid_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "\n",
        " #To add in\n",
        "    def train_multi_class_10_relu(self, training_data, labels):\n",
        "        assert len(training_data) == len(labels)\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "\n",
        "          p = np.random.permutation(len(labels))\n",
        "          training_data = training_data[p]\n",
        "          labels = labels[p]\n",
        "\n",
        "          for d in range(self.train_size):\n",
        "            #Calculate Forward Phase \n",
        "            data = training_data[d] #784*1 #print(data.shape)           \n",
        "            label = labels[d] #one hot encoded label 1*10\n",
        "            label = reshape(label,(10,1))\n",
        "\n",
        "            a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "            z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "            a1 = self.relu(z1) #28*1\n",
        "            a1 = reshape(a1,(28,1))\n",
        "\n",
        "            z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "            a2 = self.relu(z2) #28,1\n",
        "\n",
        "            z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a3 = self.relu(z3) #28,1\n",
        "\n",
        "            z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a4 = self.relu(z4) #28,1\n",
        "\n",
        "            z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a5 = self.relu(z5) #28,1\n",
        "\n",
        "            z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a6 = self.relu(z6) #28,1\n",
        "\n",
        "            z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a7 = self.relu(z7) #28,1\n",
        "\n",
        "            z8 = np.dot(self.w8, a7) + self.b8 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a8 = self.relu(z8) #28,1\n",
        "\n",
        "            z9 = np.dot(self.w9, a8) + self.b9 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a9 = self.relu(z9) #28,1\n",
        "\n",
        "            z10 = np.dot(self.w10, a9) + self.b10 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "            a10 = self.relu(z10) #28,1\n",
        "\n",
        "            z11 = np.dot(self.wout, a10) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "            output = self.relu(z11) #10,1\n",
        "\n",
        "            #evaluate the error term for each output node\n",
        "            oe11 = output - label #1*10 #syntax needs testing\n",
        "            ae11 = np.multiply(oe11, self.relu_prime(z11)) #element wise multiplication \n",
        "            # ae2 shape (10,1) output error shape (10,1), sigmoid_prime shape (10,1)\n",
        "            g11w = np.dot(ae11,a10.T) #output gradient\n",
        "            # output_gradient shape same as w2(10,28) ae2 (10,1) a1 (28,1)\n",
        "            g11b = ae11\n",
        "            #(10,1)\n",
        "\n",
        "            self.wout = self.wout - self.learning_rate*g11w\n",
        "            self.bout = self.bout - self.learning_rate*g11b\n",
        "\n",
        "            oe10 = np.dot(self.wout.T, ae11)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae10 = np.multiply(oe10,self.relu_prime(z10))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g10w = np.dot(ae10, a9.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g10b = ae10\n",
        "\n",
        "            oe9 = np.dot(self.w10.T, ae10)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae9 = np.multiply(oe9,self.relu_prime(z9))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g9w = np.dot(ae9, a8.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g9b = ae9\n",
        "\n",
        "            self.w9 = self.w9 - self.learning_rate*g9w\n",
        "            self.b9 = self.b9 - self.learning_rate*g9b\n",
        "\n",
        "            oe8 = np.dot(self.w9.T, ae9)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae8 = np.multiply(oe8,self.relu_prime(z8))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g8w = np.dot(ae8, a7.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g8b = ae8\n",
        "\n",
        "            self.w8 = self.w8 - self.learning_rate*g8w\n",
        "            self.b8 = self.b8 - self.learning_rate*g8b\n",
        "\n",
        "            oe7 = np.dot(self.w8.T, ae8)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae7 = np.multiply(oe7,self.relu_prime(z7))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g7w = np.dot(ae7, a6.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g7b = ae7\n",
        "\n",
        "            self.w7 = self.w7 - self.learning_rate*g7w\n",
        "            self.b7 = self.b7 - self.learning_rate*g7b\n",
        "\n",
        "            oe6 = np.dot(self.w7.T, ae7)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae6 = np.multiply(oe6,self.relu_prime(z6))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g6w = np.dot(ae6, a5.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g6b = ae6\n",
        "\n",
        "            self.w6 = self.w6 - self.learning_rate*g6w\n",
        "            self.b6 = self.b6 - self.learning_rate*g6b\n",
        "\n",
        "            oe5 = np.dot(self.w6.T, ae6)\n",
        "            # wout (10,28) ae3 (10,1) oe2 (28,1)\n",
        "            ae5 = np.multiply(oe5,self.relu_prime(z5))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g5w = np.dot(ae5, a4.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g5b = ae5\n",
        "\n",
        "            self.w5 = self.w5 - self.learning_rate*g5w\n",
        "            self.b5 = self.b5 - self.learning_rate*g5b\n",
        "\n",
        "            oe4 = np.dot(self.w5.T, ae5)\n",
        "            # w5 (28,28) ae5 (28,1) oe4 (28,1)\n",
        "            ae4 = np.multiply(oe4,self.relu_prime(z4))\n",
        "            # ae4 (28,1) self.sigmoid_prime(z4) (28,1)         \n",
        "            g4w = np.dot(ae4, a3.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g4b = ae4\n",
        "\n",
        "            self.w4 = self.w4 - self.learning_rate*g4w\n",
        "            self.b4 = self.b4 - self.learning_rate*g4b\n",
        "\n",
        "            oe3 = np.dot(self.w4.T, ae4)\n",
        "            # w3 (28,28) ae4 (28,1) oe3 (28,1)\n",
        "            ae3 = np.multiply(oe3,self.relu_prime(z3))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g3w = np.dot(ae3, a2.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g3b = ae3\n",
        "\n",
        "            self.w3 = self.w3 - self.learning_rate*g3w\n",
        "            self.b3 = self.b3 - self.learning_rate*g3b\n",
        "\n",
        "            oe2 = np.dot(self.w3.T, ae3)\n",
        "            # w3 (28,28) ae3 (28,1) oe2 (28,1)\n",
        "            ae2 = np.multiply(oe2,self.relu_prime(z2))\n",
        "            # ae2 (28,1) self.sigmoid_prime(z2) (28,1)         \n",
        "            g2w = np.dot(ae2, a1.T)\n",
        "            #g2w (28,28) a1 (28,1)            \n",
        "            g2b = ae2\n",
        "\n",
        "            self.w2 = self.w2 - self.learning_rate*g2w\n",
        "            self.b2 = self.b2 - self.learning_rate*g2b\n",
        "\n",
        "            oe1 = np.dot(self.w2.T, ae2)\n",
        "            # w2 (28,28) ae2 (28,1) oe1 (28,1)\n",
        "            ae1 = np.multiply(oe1,self.relu_prime(z1))\n",
        "            # ae1 (28,1) self.sigmoid_prime(z1) (28,1)         \n",
        "            g1w = np.dot(ae1, a0.T)\n",
        "            #g1w(28, 784)  ae1(28,1) a0 (784,1)            \n",
        "            #b1_partial_derivative = self.sigmoid_prime(z1) * self.w1 * self.sigmoid_prime(z2) * output_error\n",
        "            g1b = ae1\n",
        "\n",
        "            self.win = self.win - self.learning_rate*g1w\n",
        "            self.bin = self.bin - self.learning_rate*g1b\n",
        "                                                                                                             \n",
        "\n",
        "    #=========================================#\n",
        "    # Tests the prediction on each element of #\n",
        "    # the testing data. Prints the precision, #\n",
        "    # recall, and accuracy.                   #\n",
        "    #=========================================#\n",
        "    def compute_confusion_matrix(self, true, pred):\n",
        "    #'''Computes a confusion matrix using numpy for two np.arrays\n",
        "    #true and pred.\n",
        "    #\n",
        "    #Results are identical (and similar in computation time) to: \n",
        "    #  \"from sklearn.metrics import confusion_matrix\"\n",
        "    #\n",
        "    #However, this function avoids the dependency on sklearn.'''\n",
        "        K = len(true)\n",
        "        result = np.zeros((K, K))\n",
        "        for i in range(K):\n",
        "          result[int(true[i])][int(pred[i])] += 1\n",
        "        return result\n",
        "\n",
        "    def evalutation(self, confusion_matrix):\n",
        "        \n",
        "        TP = np.zeros(10)\n",
        "        FP = np.zeros(10)\n",
        "        TN = np.zeros(10)\n",
        "        FN = np.zeros(10)\n",
        "        for k in range(no_of_different_labels):#different labels\n",
        "          for i in range(no_of_different_labels): #rows #label\n",
        "            for j in range(no_of_different_labels): #columns #prediction\n",
        "              if (k == i and i == j):\n",
        "                TP[k] += confusion_matrix[i][j]\n",
        "              if (k != i and k != j):\n",
        "                TN[k] += confusion_matrix[i][j]\n",
        "              if (k != i and k == j):    \n",
        "                FP[k] += confusion_matrix[i][j]\n",
        "              if (k == i and k != j):\n",
        "                FN[k] += confusion_matrix[i][j]\n",
        "        print(\"TP:\", TP)\n",
        "        print(\"FP:\", FP)\n",
        "        print(\"TN:\", TN)\n",
        "        print(\"FN:\", FN)\n",
        "        print(\"TP+FP+TN+FN\",TP+FP+TN+FN)\n",
        "        \n",
        "        TP = np.ma.array(TP, mask=np.isnan(TP))\n",
        "        FP = np.ma.array(FP, mask=np.isnan(FP))\n",
        "        TN = np.ma.array(TN, mask=np.isnan(TN))\n",
        "        FN = np.ma.array(FN, mask=np.isnan(FN))\n",
        "\n",
        "        accuracy = 1/10 * np.sum((TP+TN)/(TP+FP+TN+FN))\n",
        "        precision = 1/10 * np.sum(TP/(TP+FP))\n",
        "        recall = 1/10 * np.sum(TP/(TP+FN))\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "        print(\"accuracy:\", accuracy)\n",
        "        print(\"precision:\", precision)\n",
        "        print(\"recall:\", recall)\n",
        "        print(\"F1 score:\", f1)\n",
        "\n",
        "    #=========================================#\n",
        "    # Tests the multi-class prediction on     #\n",
        "    # each element of the testing data.       # \n",
        "    # Prints the label,predicted value and    #\n",
        "    # accuracy of the perceptron.             #                \n",
        "    #=========================================#\n",
        "     \n",
        "    def test_multi_class_1(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.sigmoid(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.wout, a1) + self.bout # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          output = self.sigmoid(z2) #1*10\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        \n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "\n",
        "    def test_multi_class_1_relu(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.relu(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.wout, a1) + self.bout # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          output = self.relu(z2) #1*10\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "\n",
        "    def test_multi_class_2(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.sigmoid(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          a2 = self.sigmoid(z2) #1*10\n",
        "\n",
        "          z3 = np.dot(self.wout, a2) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "          output = self.sigmoid(z3) #10,1\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        \n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "    def test_multi_class_2_relu(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.relu(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "          a2 = self.relu(z2) #28,1\n",
        "\n",
        "          z3 = np.dot(self.wout, a2) + self.bout # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          output = self.relu(z3) #1*10\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "\n",
        "    def test_multi_class_3(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.sigmoid(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          a2 = self.sigmoid(z2) #1*10\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "          z4 = np.dot(self.wout, a3) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "          output = self.sigmoid(z4) #10,1\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        \n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "    def test_multi_class_3_relu(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.relu(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "          a2 = self.relu(z2) #28,1\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.relu(z3) #28,1          \n",
        "\n",
        "          z4 = np.dot(self.wout, a3) + self.bout # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          output = self.relu(z4) #1*10\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "    def test_multi_class_4(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.sigmoid(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          a2 = self.sigmoid(z2) #1*10\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "          z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a4 = self.sigmoid(z4) #28,1\n",
        "\n",
        "          z5 = np.dot(self.wout, a4) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "          output = self.sigmoid(z5) #10,1\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        \n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "\n",
        "    def test_multi_class_4_relu(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.relu(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "          a2 = self.relu(z2) #28,1\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.relu(z3) #28,1 \n",
        "\n",
        "          z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a4 = self.relu(z4) #28,1         \n",
        "\n",
        "          z5 = np.dot(self.wout, a4) + self.bout # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          output = self.relu(z5) #1*10\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "    def test_multi_class_5(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.sigmoid(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          a2 = self.sigmoid(z2) #1*10\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "          z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a4 = self.sigmoid(z4) #28,1\n",
        "\n",
        "          z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a5 = self.sigmoid(z5) #28,1\n",
        "\n",
        "          z6 = np.dot(self.wout, a5) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "          output = self.sigmoid(z6) #10,1\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        \n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "    def test_multi_class_5_relu(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.relu(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "          a2 = self.relu(z2) #28,1\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.relu(z3) #28,1 \n",
        "\n",
        "          z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a4 = self.relu(z4) #28,1\n",
        "\n",
        "          z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a5 = self.relu(z5) #28,1         \n",
        "\n",
        "          z6 = np.dot(self.wout, a5) + self.bout # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          output = self.relu(z6) #1*10\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "    def test_multi_class_6(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.sigmoid(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          a2 = self.sigmoid(z2) #1*10\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "          z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a4 = self.sigmoid(z4) #28,1\n",
        "\n",
        "          z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a5 = self.sigmoid(z5) #28,1\n",
        "\n",
        "          z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a6 = self.sigmoid(z6) #28,1\n",
        "\n",
        "          z7 = np.dot(self.wout, a6) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "          output = self.sigmoid(z7) #10,1\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        \n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "    def test_multi_class_6_relu(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.relu(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "          a2 = self.relu(z2) #28,1\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.relu(z3) #28,1 \n",
        "\n",
        "          z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a4 = self.relu(z4) #28,1         \n",
        "\n",
        "          z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a5 = self.relu(z5) #28,1\n",
        "\n",
        "          z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a6 = self.relu(z6) #28,1         \n",
        "\n",
        "          z7 = np.dot(self.wout, a6) + self.bout # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          output = self.relu(z7) #1*10\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "    def test_multi_class_7(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.sigmoid(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          a2 = self.sigmoid(z2) #1*10\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "          z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a4 = self.sigmoid(z4) #28,1\n",
        "\n",
        "          z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a5 = self.sigmoid(z5) #28,1\n",
        "\n",
        "          z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a6 = self.sigmoid(z6) #28,1\n",
        "\n",
        "          z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a7 = self.sigmoid(z7) #28,1\n",
        "\n",
        "          z8 = np.dot(self.wout, a7) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "          output = self.sigmoid(z8) #10,1\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        \n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "\n",
        "    def test_multi_class_7_relu(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.relu(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "          a2 = self.relu(z2) #28,1\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.relu(z3) #28,1 \n",
        "\n",
        "          z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a4 = self.relu(z4) #28,1         \n",
        "\n",
        "          z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a5 = self.relu(z5) #28,1\n",
        "\n",
        "          z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a6 = self.relu(z6) #28,1         \n",
        "\n",
        "          z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a7 = self.relu(z7) #28,1\n",
        "\n",
        "          z8 = np.dot(self.wout, a7) + self.bout # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          output = self.relu(z8) #1*10\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "    def test_multi_class_8(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.sigmoid(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          a2 = self.sigmoid(z2) #1*10\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "          z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a4 = self.sigmoid(z4) #28,1\n",
        "\n",
        "          z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a5 = self.sigmoid(z5) #28,1\n",
        "\n",
        "          z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a6 = self.sigmoid(z6) #28,1\n",
        "\n",
        "          z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a7 = self.sigmoid(z7) #28,1\n",
        "\n",
        "          z8 = np.dot(self.w8, a7) + self.b8 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a8 = self.sigmoid(z8) #28,1\n",
        "\n",
        "          z9 = np.dot(self.wout, a8) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "          output = self.sigmoid(z9) #10,1\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        \n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "    def test_multi_class_8_relu(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.relu(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "          a2 = self.relu(z2) #28,1\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.relu(z3) #28,1 \n",
        "\n",
        "          z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a4 = self.relu(z4) #28,1         \n",
        "\n",
        "          z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a5 = self.relu(z5) #28,1\n",
        "\n",
        "          z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a6 = self.relu(z6) #28,1         \n",
        "\n",
        "          z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a7 = self.relu(z7) #28,1\n",
        "\n",
        "          z8 = np.dot(self.w8, a7) + self.b8 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a8 = self.relu(z8) #28,1\n",
        "\n",
        "          z9 = np.dot(self.wout, a8) + self.bout # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          output = self.relu(z9) #1*10\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "    def test_multi_class_9(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.sigmoid(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          a2 = self.sigmoid(z2) #1*10\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "          z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a4 = self.sigmoid(z4) #28,1\n",
        "\n",
        "          z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a5 = self.sigmoid(z5) #28,1\n",
        "\n",
        "          z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a6 = self.sigmoid(z6) #28,1\n",
        "\n",
        "          z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a7 = self.sigmoid(z7) #28,1\n",
        "\n",
        "          z8 = np.dot(self.w8, a7) + self.b8 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a8 = self.sigmoid(z8) #28,1\n",
        "\n",
        "          z9 = np.dot(self.w9, a8) + self.b9 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a9 = self.sigmoid(z9) #28,1\n",
        "\n",
        "          z10 = np.dot(self.wout, a9) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "          output = self.sigmoid(z10) #10,1\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        \n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "    def test_multi_class_9_relu(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.relu(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "          a2 = self.relu(z2) #28,1\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.relu(z3) #28,1 \n",
        "\n",
        "          z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a4 = self.relu(z4) #28,1         \n",
        "\n",
        "          z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a5 = self.relu(z5) #28,1\n",
        "\n",
        "          z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a6 = self.relu(z6) #28,1         \n",
        "\n",
        "          z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a7 = self.relu(z7) #28,1\n",
        "\n",
        "          z8 = np.dot(self.w8, a7) + self.b8 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a8 = self.relu(z8) #28,1\n",
        "\n",
        "          z9 = np.dot(self.w9, a8) + self.b9 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a9 = self.relu(z9) #28,1\n",
        "\n",
        "          z10 = np.dot(self.wout, a9) + self.bout # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          output = self.relu(z10) #1*10\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "    def test_multi_class_10(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.sigmoid(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          a2 = self.sigmoid(z2) #1*10\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.sigmoid(z3) #28,1\n",
        "\n",
        "          z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a4 = self.sigmoid(z4) #28,1\n",
        "\n",
        "          z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a5 = self.sigmoid(z5) #28,1\n",
        "\n",
        "          z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a6 = self.sigmoid(z6) #28,1\n",
        "\n",
        "          z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a7 = self.sigmoid(z7) #28,1\n",
        "\n",
        "          z8 = np.dot(self.w8, a7) + self.b8 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a8 = self.sigmoid(z8) #28,1\n",
        "\n",
        "          z9 = np.dot(self.w9, a8) + self.b9 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a9 = self.sigmoid(z9) #28,1\n",
        "\n",
        "          z10 = np.dot(self.w10, a9) + self.b10 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a10 = self.sigmoid(z10) #28,1\n",
        "\n",
        "          z11 = np.dot(self.wout, a10) + self.bout # (10,28)dot(28,1) + (10,1) = 10*1\n",
        "          output = self.sigmoid(z11) #10,1\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        \n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n",
        "\n",
        "    def test_multi_class_10_relu(self, testing_data, labels):\n",
        "        assert len(testing_data) == len(labels)\n",
        "\n",
        "        predict = np.zeros(self.test_size)\n",
        "        actual = np.zeros(self.test_size)\n",
        "\n",
        "        for d in range(self.test_size):\n",
        "          #Calculate Forward Phase \n",
        "          data = testing_data[d] #784*1 #print(data.shape)           \n",
        "          label = labels[d] #one hot encoded label\n",
        "\n",
        "          label = reshape(label,(10,1))\n",
        "\n",
        "          a0 = reshape(data,(784,1)) #784*1\n",
        "\n",
        "          z1 = np.dot(self.win, a0) + self.bin # (28, 784)dot(784,1) + (28*1) = 28*1\n",
        "\n",
        "          a1 = self.relu(z1) #28*1\n",
        "          a1 = reshape(a1,(28,1))\n",
        "\n",
        "          z2 = np.dot(self.w2, a1) + self.b2 # (28,28) (28,1) + (28,1) = 28*1\n",
        "          a2 = self.relu(z2) #28,1\n",
        "\n",
        "          z3 = np.dot(self.w3, a2) + self.b3 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a3 = self.relu(z3) #28,1 \n",
        "\n",
        "          z4 = np.dot(self.w4, a3) + self.b4 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a4 = self.relu(z4) #28,1         \n",
        "\n",
        "          z5 = np.dot(self.w5, a4) + self.b5 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a5 = self.relu(z5) #28,1\n",
        "\n",
        "          z6 = np.dot(self.w6, a5) + self.b6 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a6 = self.relu(z6) #28,1         \n",
        "\n",
        "          z7 = np.dot(self.w7, a6) + self.b7 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a7 = self.relu(z7) #28,1\n",
        "\n",
        "          z8 = np.dot(self.w8, a7) + self.b8 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a8 = self.relu(z8) #28,1\n",
        "\n",
        "          z9 = np.dot(self.w9, a8) + self.b9 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a9 = self.relu(z9) #28,1\n",
        "\n",
        "          z10 = np.dot(self.w10, a9) + self.b10 # (28,28)dot(28,1) + (28,1) = 28*1\n",
        "          a10 = self.relu(z10) #28,1\n",
        "\n",
        "          z11 = np.dot(self.wout, a10) + self.bout # (10*28)dot(28*1) + (10*1) = 10*1\n",
        "          output = self.relu(z11) #1*10\n",
        "\n",
        "          output = np.asarray(output)\n",
        "          max = np.amax(output)\n",
        "          prediction = np.asarray([int (x==max) for x in output])\n",
        "\n",
        "          predict[d] = int(np.argmax(prediction))#predicted digit/label\n",
        "          actual[d] = int(np.argmax(label))#actual digit/label\n",
        "\n",
        "        print(\"predit\",predict)\n",
        "        print(\"actual\",actual)\n",
        "        confusion_matrix = self.compute_confusion_matrix(actual,predict)\n",
        "        return self.evalutation(confusion_matrix)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FajqJzEvwd0Q"
      },
      "source": [
        "1min 37sec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lFhwlWo_NOe"
      },
      "source": [
        "##Implementation of the main method(Q2.1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVibnKpN98nP"
      },
      "source": [
        "#Load training data\n",
        "#Load testing data\n",
        "image_size = 28\n",
        "no_of_different_labels = 10\n",
        "image_pixels = image_size * image_size\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/DLNNAssignment2/\"\n",
        "train_data_10 = np.loadtxt(data_path + \"mnist_train_10.csv\", delimiter = \",\")\n",
        "train_data = np.loadtxt(data_path + \"mnist_train.csv\", delimiter = \",\")\n",
        "test_data = np.loadtxt(data_path + \"mnist_test.csv\", delimiter = \",\")\n",
        "\n",
        "fac = 0.99 / 255\n",
        "train_imgs_10 = np.asfarray(train_data_10[:, 1:]) * fac + 0.01\n",
        "train_imgs = np.asfarray(train_data[:, 1:]) * fac + 0.01\n",
        "test_imgs = np.asfarray(test_data[:, 1:]) * fac + 0.01\n",
        "\n",
        "train_labels_10 = np.asfarray(train_data_10[:, :1])\n",
        "train_labels = np.asfarray(train_data[:, :1])\n",
        "test_labels = np.asfarray(test_data[:, :1])\n",
        "\n",
        "#one hot encoding labels\n",
        "lr = np.arange(no_of_different_labels)\n",
        "\n",
        "#transform labels into one hot representation\n",
        "train_labels_10_one_hot = (lr==train_labels_10).astype(np.float)\n",
        "train_labels_one_hot = (lr==train_labels).astype(np.float)\n",
        "test_labels_one_hot = (lr==test_labels).astype(np.float)\n",
        "\n",
        "#we don't want zeroes and ones in the labels neither:\n",
        "train_labels_10_one_hot[train_labels_10_one_hot==0] = 0.01\n",
        "train_labels_10_one_hot[train_labels_10_one_hot==1] = 0.99\n",
        "train_labels_one_hot[train_labels_one_hot==0] = 0.01\n",
        "train_labels_one_hot[train_labels_one_hot==1] = 0.99\n",
        "test_labels_one_hot[test_labels_one_hot==0] = 0.01\n",
        "test_labels_one_hot[test_labels_one_hot==1] = 0.99"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF5O93mFwica"
      },
      "source": [
        "35sec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsBsJimJ6Mba"
      },
      "source": [
        "###1 hidden layer sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GetSrSZ2-PCs"
      },
      "source": [
        "#Creat a net\n",
        "net1_sigmoid = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSCk_bCl-yte"
      },
      "source": [
        "#Call train \n",
        "net1_sigmoid.train_multi_class_1(train_imgs,train_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPd5gGfHtjm1"
      },
      "source": [
        "~4.5min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtHg0ldo-6Go",
        "outputId": "66c3aa67-ffd5-49eb-d398-b53e55f8d6ea"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net1_sigmoid.test_multi_class_1(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [7. 2. 1. ... 7. 5. 5.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [ 888. 1112.  809.    9.   83.  522.    0.  749.   25.    0.]\n",
            "FP: [ 290.   68.  418.   24.  485.  884.    0. 3576.   58.    0.]\n",
            "TN: [8730. 8797. 8550. 8966. 8533. 8224. 9042. 5396. 8968. 8991.]\n",
            "FN: [  92.   23.  223. 1001.  899.  370.  958.  279.  949. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8839400000000002\n",
            "precision: 0.36200286648174296\n",
            "recall: 0.4102673503140191\n",
            "F1 score: 0.3846269184217991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PTkdkrBHHxw"
      },
      "source": [
        "####Change Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWkiJ-7dG9ws",
        "outputId": "5bf0d572-5c3f-46f8-a5dd-b52810af3ddc"
      },
      "source": [
        "#Creat a net\n",
        "net1_sigmoid_l2 = ANN(image_size * image_size, learning_rate= 0.01, max_iterations= 20, train_size= 60000, test_size= 10000)\n",
        "\n",
        "#Call train \n",
        "net1_sigmoid_l2.train_multi_class_1(train_imgs,train_labels_one_hot)\n",
        "\n",
        "net1_sigmoid_l2.test_multi_class_1(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [7. 6. 1. ... 9. 9. 6.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [ 949. 1092.  721.  134.   32.  183.  854.  931.  297.  489.]\n",
            "FP: [518. 176. 454. 524.  94. 238. 314. 562. 596. 842.]\n",
            "TN: [8502. 8689. 8514. 8466. 8924. 8870. 8728. 8410. 8430. 8149.]\n",
            "FN: [ 31.  43. 311. 876. 950. 709. 104.  97. 677. 520.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.91364\n",
            "precision: 0.5068729995462399\n",
            "recall: 0.5586190982786301\n",
            "F1 score: 0.5314895127356353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDG88FVIHG5P",
        "outputId": "8ba015eb-0cc0-4cde-9ce3-2b1f55e8b566"
      },
      "source": [
        "#Creat a net\n",
        "net1_sigmoid_l3 = ANN(image_size * image_size, learning_rate= 0.001, max_iterations= 20, train_size= 60000, test_size= 10000)\n",
        "\n",
        "#Call train \n",
        "net1_sigmoid_l3.train_multi_class_1(train_imgs,train_labels_one_hot)\n",
        "\n",
        "net1_sigmoid_l3.test_multi_class_1(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [1. 1. 1. ... 1. 1. 1.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [  58. 1135.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "FP: [  30. 8777.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [8990.   88. 8968. 8990. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [ 922.    0. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.82386\n",
            "precision: 0.07735985765646783\n",
            "recall: 0.10591836734693878\n",
            "F1 score: 0.08941411148009923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XodCwwNdHKuK"
      },
      "source": [
        "###Change Max Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLnM8cUaKFXJ",
        "outputId": "187164bf-aa6c-48f9-906a-6ddd3c2710ff"
      },
      "source": [
        "#Creat a net\n",
        "net1_sigmoid_l2_i2 = ANN(image_size * image_size, learning_rate= 0.01, max_iterations= 50, train_size= 60000, test_size= 10000)\n",
        "\n",
        "#Call train \n",
        "net1_sigmoid_l2_i2.train_multi_class_1(train_imgs,train_labels_one_hot)\n",
        "\n",
        "net1_sigmoid_l2_i2.test_multi_class_1(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [7. 0. 1. ... 4. 6. 3.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [ 910. 1117.    0.  575.  864.    0.   76.  229.    6.  695.]\n",
            "FP: [ 491.   65.    0. 1814.  383.    0.  356. 1052.    7. 1360.]\n",
            "TN: [8529. 8800. 8968. 7176. 8635. 9108. 8686. 7920. 9019. 7631.]\n",
            "FN: [  70.   18. 1032.  435.  118.  892.  882.  799.  968.  314.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.88944\n",
            "precision: 0.3682524345956335\n",
            "recall: 0.43589119402318216\n",
            "F1 score: 0.3992271721248047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbY6xUA5LzSS"
      },
      "source": [
        "###Change Weight Initialisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpduePfiL3C-"
      },
      "source": [
        "The code does not include the flexibility to change weight initialisation when creating new instances for the class. Therefore the change was made through the class implementation.  \\\n",
        "\n",
        "In the tests before, the initial weights were a constant close to zero (1/(10*784)). \\\n",
        "\n",
        "In the test below to change weights, the weight was changed to using He initialisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_wFD7c9MK1Z",
        "outputId": "1d10e97e-6821-4c62-e53e-2fca2c24e242"
      },
      "source": [
        "#Creat a net\n",
        "net1_sigmoid_l2 = ANN(image_size * image_size, learning_rate= 0.01, max_iterations= 20, train_size= 60000, test_size= 10000)\n",
        "\n",
        "#Call train \n",
        "net1_sigmoid_l2.train_multi_class_1(train_imgs,train_labels_one_hot)\n",
        "\n",
        "net1_sigmoid_l2.test_multi_class_1(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [7. 2. 1. ... 4. 5. 6.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [ 967. 1116.  956.  944.  936.  833.  919.  975.  928.  933.]\n",
            "FP: [48. 24. 57. 46. 57. 49. 54. 48. 59. 51.]\n",
            "TN: [8972. 8841. 8911. 8944. 8961. 9059. 8988. 8924. 8967. 8940.]\n",
            "FN: [13. 19. 76. 66. 46. 59. 39. 53. 46. 76.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.9901399999999999\n",
            "precision: 0.9501940553826351\n",
            "recall: 0.950320172580815\n",
            "F1 score: 0.9502571097971871\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES2zmcCtqk4m"
      },
      "source": [
        "###Change Activation Function and Weight Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5wHX3UVPvAF",
        "outputId": "ba0afd5a-3e85-4644-ae6a-8acce54da6c3"
      },
      "source": [
        "#Creat a net\n",
        "net1_relu_l2 = ANN(image_size * image_size, learning_rate= 0.01, max_iterations= 20, train_size= 60000, test_size= 10000)\n",
        "\n",
        "#Call train \n",
        "net1_relu_l2.train_multi_class_1_relu(train_imgs,train_labels_one_hot)\n",
        "\n",
        "net1_relu_l2.test_multi_class_1_relu(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [0. 0. 0. ... 0. 0. 0.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [980.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "FP: [9020.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [   0. 8865. 8968. 8990. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [   0. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8196\n",
            "precision: 0.009800000000000001\n",
            "recall: 0.1\n",
            "F1 score: 0.01785063752276867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCDjbUYpsega"
      },
      "source": [
        "###Change Learning Rate, Activation Function, Weight Initialisation and Max Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s85mupCY_4G",
        "outputId": "260de214-853c-48fe-fc75-0ea99c6410fb"
      },
      "source": [
        "#Creat a net\n",
        "net1_relu_l2 = ANN(image_size * image_size, learning_rate= 0.0001, max_iterations= 50, train_size= 60000, test_size= 10000)\n",
        "\n",
        "#Call train \n",
        "net1_relu_l2.train_multi_class_1_relu(train_imgs,train_labels_one_hot)\n",
        "\n",
        "net1_relu_l2.test_multi_class_1_relu(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [7. 2. 1. ... 4. 5. 6.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [ 963. 1116.  977.  947.  933.  828.  921.  980.  924.  915.]\n",
            "FP: [53. 25. 48. 49. 67. 50. 59. 43. 59. 43.]\n",
            "TN: [8967. 8840. 8920. 8941. 8951. 9058. 8983. 8929. 8967. 8948.]\n",
            "FN: [17. 19. 55. 63. 49. 64. 37. 48. 50. 94.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.9900800000000001\n",
            "precision: 0.9498807536747607\n",
            "recall: 0.9498784131137538\n",
            "F1 score: 0.9498795833928153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhqNum02qusV"
      },
      "source": [
        "training time: 16min57s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIU8ZjmBoKta"
      },
      "source": [
        "##Change the number of hidden layers and/or activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrx5vuBF6aR7"
      },
      "source": [
        "###1 hidden layer relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH9P565I6aR8"
      },
      "source": [
        "#Creat a net\n",
        "net1_relu = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_0e45Iz6aSE"
      },
      "source": [
        "#Call train \n",
        "net1_relu.train_multi_class_1_relu(train_imgs,train_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0Z724z_tf47"
      },
      "source": [
        "~5.5min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzawtmpX6aSF",
        "outputId": "b0246520-f3e0-4c84-998f-537bbabf4cf6"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net1_relu.test_multi_class_1_relu(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [0. 0. 0. ... 0. 0. 0.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [980.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "FP: [9020.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [   0. 8865. 8968. 8990. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [   0. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8196\n",
            "precision: 0.009800000000000001\n",
            "recall: 0.1\n",
            "F1 score: 0.01785063752276867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nhx1d1om5m9W"
      },
      "source": [
        "###2 hidden layers sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hng0e7Jl5kzW"
      },
      "source": [
        "#Creat a net\n",
        "net2_sigmoid = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUI70KKT5kzi"
      },
      "source": [
        "#Call train \n",
        "net2_sigmoid.train_multi_class_2(train_imgs,train_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWbOIHpCtJQ5"
      },
      "source": [
        "~4.5min "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md6rmSxF5kzj",
        "outputId": "d7afe381-9d96-4a93-e98a-391b8abc4dc8"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net2_sigmoid.test_multi_class_2(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [8. 8. 1. ... 8. 8. 8.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [   0. 1117.    0.    0.    0.    0.    0.    0.  956.    0.]\n",
            "FP: [   0.   56.    0.    0.    0.    0.    0.    0. 7871.    0.]\n",
            "TN: [9020. 8809. 8968. 8990. 9018. 9108. 9042. 8972. 1155. 8991.]\n",
            "FN: [ 980.   18. 1032. 1010.  982.  892.  958. 1028.   18. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8414600000000002\n",
            "precision: 0.1060563231602333\n",
            "recall: 0.19656604763498542\n",
            "F1 score: 0.1377761479795743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ7IHK_86EKD"
      },
      "source": [
        "###2 hidden layers relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWsKQjBP6EKR"
      },
      "source": [
        "#Creat a net\n",
        "net2_relu = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYKQel7V6EKR"
      },
      "source": [
        "#Call train \n",
        "net2_relu.train_multi_class_2_relu(train_imgs,train_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUwR5XNUs3Jj"
      },
      "source": [
        "10min training time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpUGF7nQ6EKS",
        "outputId": "2b6fbfc3-2c8c-434b-f685-39c0578798f4"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net2_relu.test_multi_class_2_relu(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [0. 0. 0. ... 0. 0. 0.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [980.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "FP: [9020.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [   0. 8865. 8968. 8990. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [   0. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8196\n",
            "precision: 0.009800000000000001\n",
            "recall: 0.1\n",
            "F1 score: 0.01785063752276867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJJlzQgKYRqj"
      },
      "source": [
        "#Creat a net\n",
        "net2_relu_l2 = ANN(image_size * image_size, learning_rate= 0.01, max_iterations= 20, train_size= 60000, test_size= 10000)\n",
        "\n",
        "#Call train \n",
        "net2_relu_l2.train_multi_class_2_relu(train_imgs,train_labels_one_hot)\n",
        "\n",
        "net2_relu_l2.test_multi_class_2_relu(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hx2woZp7dyT"
      },
      "source": [
        "###3 hidden layers sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGyc2UAN7dyf"
      },
      "source": [
        "#Creat a net\n",
        "net3_sigmoid = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyK2LpRH7dyg"
      },
      "source": [
        "#Call train \n",
        "net3_sigmoid.train_multi_class_2(train_imgs,train_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9DlSWPWtnKR"
      },
      "source": [
        "4min 5 sec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "725N41MW7dyg",
        "outputId": "b807e5ff-4c82-415b-ab37-dec1c49d6cf9"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net3_sigmoid.test_multi_class_2(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [3. 3. 1. ... 3. 3. 3.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [   0. 1115.    0. 1009.    0.    0.    0.    0.    0.    0.]\n",
            "FP: [   0.   49.    0. 7827.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [9020. 8816. 8968. 1163. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [9.800e+02 2.000e+01 1.032e+03 1.000e+00 9.820e+02 8.920e+02 9.580e+02\n",
            " 1.028e+03 9.740e+02 1.009e+03]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8424800000000001\n",
            "precision: 0.10720957221239572\n",
            "recall: 0.19813887556156495\n",
            "F1 score: 0.13913536644748573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rGhDFtU7dyh"
      },
      "source": [
        "###3 hidden layers relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm7pLjYv7dyh"
      },
      "source": [
        "#Creat a net\n",
        "net3_relu = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDPNnoyl7dyh"
      },
      "source": [
        "\n",
        "#Call train \n",
        "net3_relu.train_multi_class_3_relu(train_imgs,train_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqlVTokdt_3P"
      },
      "source": [
        "~13min45sec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoHoEKx-7dyi",
        "outputId": "f966371a-ab48-4900-f0ef-5c698572806d"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net3_relu.test_multi_class_3_relu(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [0. 0. 0. ... 0. 0. 0.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [980.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "FP: [9020.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [   0. 8865. 8968. 8990. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [   0. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8196\n",
            "precision: 0.009800000000000001\n",
            "recall: 0.1\n",
            "F1 score: 0.01785063752276867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THz9fjX18IHR"
      },
      "source": [
        "###4 hidden layers sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7QI8YpU8IHb"
      },
      "source": [
        "#Creat a net\n",
        "net4_sigmoid = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eH-lJY28IHc",
        "outputId": "4aa396b7-27ee-4d93-966e-b3690302a10e"
      },
      "source": [
        "#Call train \n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "print(\"training...\")\n",
        "net4_sigmoid.train_multi_class_4(train_imgs,train_labels_one_hot)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "365.31356477737427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C64rK5T8IHc",
        "outputId": "e5b45934-5a4e-4878-d1db-b4647288424a"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net4_sigmoid.test_multi_class_4(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [4. 4. 4. ... 4. 4. 4.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [  0.   0.   0.   0. 982.   0.   0.   0.   0.   0.]\n",
            "FP: [   0.    0.    0.    0. 9018.    0.    0.    0.    0.    0.]\n",
            "TN: [9020. 8865. 8968. 8990.    0. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [ 980. 1135. 1032. 1010.    0.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8196400000000001\n",
            "precision: 0.00982\n",
            "recall: 0.1\n",
            "F1 score: 0.017883809870697504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF_ZdkgV8IHd"
      },
      "source": [
        "###4 hidden layers relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjijyMEA8IHd"
      },
      "source": [
        "#Creat a net\n",
        "net4_relu = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUldcciv8IHd",
        "outputId": "b5a11822-aa34-4951-a849-49c2c17bd97d"
      },
      "source": [
        "#Call train \n",
        "import time\n",
        "start = time.time()\n",
        "print(\"training...\")\n",
        "net4_relu.train_multi_class_4_relu(train_imgs,train_labels_one_hot)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "1046.9254002571106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLo4x9zr8IHd",
        "outputId": "fd54fcab-8b5b-4f28-8e28-a8c533801a78"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net4_relu.test_multi_class_4_relu(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [0. 0. 0. ... 0. 0. 0.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [980.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "FP: [9020.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [   0. 8865. 8968. 8990. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [   0. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8196\n",
            "precision: 0.009800000000000001\n",
            "recall: 0.1\n",
            "F1 score: 0.01785063752276867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY7OBuif9Vox"
      },
      "source": [
        "###5 hidden layers sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCUTZ0oO9Vo1"
      },
      "source": [
        "#Creat a net\n",
        "net5_sigmoid = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQdhgF7-9Vo1",
        "outputId": "a5dd6e5c-f78d-4a9e-874e-8fc7573437ac"
      },
      "source": [
        "#Call train \n",
        "import time\n",
        "start = time.time()\n",
        "print(\"training...\")\n",
        "net5_sigmoid.train_multi_class_5(train_imgs,train_labels_one_hot)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "431.53079986572266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGM4CAh69Vo1",
        "outputId": "fde86360-ab93-4e67-d71d-d5b568ae72a9"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net5_sigmoid.test_multi_class_5(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [1. 1. 1. ... 1. 1. 1.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [   0. 1135.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "FP: [   0. 8865.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [9020.    0. 8968. 8990. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [ 980.    0. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8227000000000001\n",
            "precision: 0.01135\n",
            "recall: 0.1\n",
            "F1 score: 0.0203861697350696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxBzgl2O9Vo2"
      },
      "source": [
        "###5 hidden layers relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-4iONLm9Vo2"
      },
      "source": [
        "#Creat a net\n",
        "net5_relu = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPAi81IS9Vo3",
        "outputId": "52371938-69b9-4ef3-b5bb-fdeb0be5da64"
      },
      "source": [
        "#Call train \n",
        "import time\n",
        "start = time.time()\n",
        "print(\"training...\")\n",
        "net5_relu.train_multi_class_5_relu(train_imgs,train_labels_one_hot)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "1260.599053144455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nQFsqpH29Vo3",
        "outputId": "49f5b5e2-3cde-4188-8853-aa52c79d9636"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net5_relu.test_multi_class_5_relu(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [0. 0. 0. ... 0. 0. 0.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [980.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "FP: [9020.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [   0. 8865. 8968. 8990. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [   0. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8196\n",
            "precision: 0.009800000000000001\n",
            "recall: 0.1\n",
            "F1 score: 0.01785063752276867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIsxz8_A9nWM"
      },
      "source": [
        "###6 hidden layers sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AimNEWWq9nWa"
      },
      "source": [
        "#Creat a net\n",
        "net6_sigmoid = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bQwh243z9nWb",
        "outputId": "d1b1f2ff-36c2-426b-c93f-a824fa12ab05"
      },
      "source": [
        "#Call train \n",
        "import time\n",
        "start = time.time()\n",
        "print(\"training...\")\n",
        "net6_sigmoid.train_multi_class_6(train_imgs,train_labels_one_hot)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "486.9436249732971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S8P7QBg49nWc",
        "outputId": "b58bc055-f08a-4bac-dac0-3901222c9ab9"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net6_sigmoid.test_multi_class_6(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [5. 5. 5. ... 5. 5. 5.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [  0.   0.   0.   0.   0. 892.   0.   0.   0.   0.]\n",
            "FP: [   0.    0.    0.    0.    0. 9108.    0.    0.    0.    0.]\n",
            "TN: [9020. 8865. 8968. 8990. 9018.    0. 9042. 8972. 9026. 8991.]\n",
            "FN: [ 980. 1135. 1032. 1010.  982.    0.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.81784\n",
            "precision: 0.00892\n",
            "recall: 0.1\n",
            "F1 score: 0.016378993756885788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa-HAQsV9nWd"
      },
      "source": [
        "###6 hidden layers relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBf5DMNV9nWd"
      },
      "source": [
        "#Creat a net\n",
        "net6_relu = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSU7NxQB9nWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b89a5e2-6599-4def-86e8-534ecdf6fff6"
      },
      "source": [
        "#Call train \n",
        "import time\n",
        "start = time.time()\n",
        "print(\"training...\")\n",
        "net6_relu.train_multi_class_6_relu(train_imgs,train_labels_one_hot)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "1796.9338972568512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIU6JnSw9nWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093846ed-b7c9-4f9a-f35a-a5fd07d0fc45"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net6_relu.test_multi_class_6_relu(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [0. 0. 0. ... 0. 0. 0.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [980.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "FP: [9020.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [   0. 8865. 8968. 8990. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [   0. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8196\n",
            "precision: 0.009800000000000001\n",
            "recall: 0.1\n",
            "F1 score: 0.01785063752276867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqYZBphU95j9"
      },
      "source": [
        "###7 hidden layers sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AgFFfwO95kJ"
      },
      "source": [
        "#Creat a net\n",
        "net7_sigmoid = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEirh2g595kK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a4bad1e-c4ef-45da-c99a-ae2a71184005"
      },
      "source": [
        "#Call train \n",
        "import time\n",
        "start = time.time()\n",
        "print(\"training...\")\n",
        "net7_sigmoid.train_multi_class_7(train_imgs,train_labels_one_hot)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "715.8330142498016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMYrd6M095kK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811e5e94-1c5b-4f01-9175-7a6eb4d76911"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net7_sigmoid.test_multi_class_7(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [3. 3. 3. ... 3. 3. 3.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [   0.    0.    0. 1010.    0.    0.    0.    0.    0.    0.]\n",
            "FP: [   0.    0.    0. 8990.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [9020. 8865. 8968.    0. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [ 980. 1135. 1032.    0.  982.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8202\n",
            "precision: 0.010100000000000001\n",
            "recall: 0.1\n",
            "F1 score: 0.018346957311534972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da9zulP895kL"
      },
      "source": [
        "###7 hidden layers relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJj0jqL995kM"
      },
      "source": [
        "#Creat a net\n",
        "net7_relu = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daQ71t0O95kM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c14197-3e27-46ae-96d5-da6f0fefc198"
      },
      "source": [
        "#Call train \n",
        "import time\n",
        "start = time.time()\n",
        "print(\"training...\")\n",
        "net7_relu.train_multi_class_7_relu(train_imgs,train_labels_one_hot)\n",
        "end = time.time()\n",
        "print(end - start)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "2144.5411286354065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U-41pfz95kM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66684bda-460f-4643-cab9-8e4f972ece10"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net7_relu.test_multi_class_7_relu(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [0. 0. 0. ... 0. 0. 0.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [980.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "FP: [9020.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [   0. 8865. 8968. 8990. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [   0. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8196\n",
            "precision: 0.009800000000000001\n",
            "recall: 0.1\n",
            "F1 score: 0.01785063752276867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g7GzosP-VLy"
      },
      "source": [
        "###8 hidden layers sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze1w_C-l-VL1"
      },
      "source": [
        "#Creat a net\n",
        "net8_sigmoid = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTCdVwCp-VL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1292e987-5fd1-4b7b-b5eb-917d263ff94d"
      },
      "source": [
        "#Call train \n",
        "import time\n",
        "start = time.time()\n",
        "print(\"training...\")\n",
        "net8_sigmoid.train_multi_class_8(train_imgs,train_labels_one_hot)\n",
        "end = time.time()\n",
        "print(end - start)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "803.2739796638489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlcfwXzY-VL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e4a7ae-63d2-49ac-f09b-c0ddf3ba9af4"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net8_sigmoid.test_multi_class_8(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [0. 0. 0. ... 0. 0. 0.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [980.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "FP: [9020.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [   0. 8865. 8968. 8990. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [   0. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8196\n",
            "precision: 0.009800000000000001\n",
            "recall: 0.1\n",
            "F1 score: 0.01785063752276867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ormr0PI-VL2"
      },
      "source": [
        "###8 hidden layers relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o0h8Cc2-VL2"
      },
      "source": [
        "#Creat a net\n",
        "net8_relu = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkK23jhB-VL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41dea9e8-3911-4f6e-c8be-1f26c26bcca1"
      },
      "source": [
        "#Call train\n",
        "import time\n",
        "start = time.time()\n",
        "print(\"training...\")\n",
        "net8_relu.train_multi_class_8_relu(train_imgs,train_labels_one_hot)\n",
        "end = time.time()\n",
        "print(end - start) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "2332.8792560100555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA7aHO2v-VL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf19a741-9d7a-4c02-f903-60ad32429cdb"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net8_relu.test_multi_class_8_relu(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [0. 0. 0. ... 0. 0. 0.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [980.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "FP: [9020.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [   0. 8865. 8968. 8990. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [   0. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8196\n",
            "precision: 0.009800000000000001\n",
            "recall: 0.1\n",
            "F1 score: 0.01785063752276867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK2gA9BX-ozR"
      },
      "source": [
        "###9 hidden layers sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhVLbF12-ozS"
      },
      "source": [
        "#Creat a net\n",
        "net9_sigmoid = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCLg_g5q-ozS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29303be6-17da-4358-b97a-f6646e5c20c5"
      },
      "source": [
        "#Call train \n",
        "import time\n",
        "start = time.time()\n",
        "print(\"training...\")\n",
        "net9_sigmoid.train_multi_class_9(train_imgs,train_labels_one_hot)\n",
        "end = time.time()\n",
        "print(end - start)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "898.1824674606323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzCFzGSP-ozS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e363b406-383b-48de-e303-be67340615e9"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net9_sigmoid.test_multi_class_9(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [9. 9. 9. ... 9. 9. 9.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [   0.    0.    0.    0.    0.    0.    0.    0.    0. 1009.]\n",
            "FP: [   0.    0.    0.    0.    0.    0.    0.    0.    0. 8991.]\n",
            "TN: [9020. 8865. 8968. 8990. 9018. 9108. 9042. 8972. 9026.    0.]\n",
            "FN: [ 980. 1135. 1032. 1010.  982.  892.  958. 1028.  974.    0.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8201799999999999\n",
            "precision: 0.010090000000000002\n",
            "recall: 0.1\n",
            "F1 score: 0.0183304568989009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdT89HB_-ozT"
      },
      "source": [
        "###9 hidden layers relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pyg9H2i-ozT"
      },
      "source": [
        "#Creat a net\n",
        "net9_relu = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJdv0-Ao-ozT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39895150-55cb-48e1-f84d-5b0be92cdc4c"
      },
      "source": [
        "#Call train \n",
        "import time\n",
        "start = time.time()\n",
        "print(\"training...\")\n",
        "net9_relu.train_multi_class_9_relu(train_imgs,train_labels_one_hot)\n",
        "end = time.time()\n",
        "print(end - start)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "2700.466679573059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G7wxTF6-ozT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bdbe1c0-8dd0-4677-d0ca-0ddd6ddb089e"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net9_relu.test_multi_class_9_relu(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [0. 0. 0. ... 0. 0. 0.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [980.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "FP: [9020.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
            "TN: [   0. 8865. 8968. 8990. 9018. 9108. 9042. 8972. 9026. 8991.]\n",
            "FN: [   0. 1135. 1032. 1010.  982.  892.  958. 1028.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8196\n",
            "precision: 0.009800000000000001\n",
            "recall: 0.1\n",
            "F1 score: 0.01785063752276867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT0X-jjV_Ic-"
      },
      "source": [
        "###10 hidden layers sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO8ducUl_IdJ"
      },
      "source": [
        "#Creat a net\n",
        "net10_sigmoid = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X39lAQT-_IdJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1990db3-b689-475d-ba0d-a0c49bfbd866"
      },
      "source": [
        "#Call train \n",
        "import time\n",
        "start = time.time()\n",
        "print(\"training...\")\n",
        "net10_sigmoid.train_multi_class_10(train_imgs,train_labels_one_hot)\n",
        "end = time.time()\n",
        "print(end - start)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "966.9217977523804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lZb3tvn_IdJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d46b63-9aa4-421a-97b0-b68c5f819e4d"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net10_sigmoid.test_multi_class_10(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [7. 7. 7. ... 7. 7. 7.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [   0.    0.    0.    0.    0.    0.    0. 1028.    0.    0.]\n",
            "FP: [   0.    0.    0.    0.    0.    0.    0. 8972.    0.    0.]\n",
            "TN: [9020. 8865. 8968. 8990. 9018. 9108. 9042.    0. 9026. 8991.]\n",
            "FN: [ 980. 1135. 1032. 1010.  982.  892.  958.    0.  974. 1009.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8205600000000001\n",
            "precision: 0.010280000000000001\n",
            "recall: 0.1\n",
            "F1 score: 0.018643453028654333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okjohaOH_IdK"
      },
      "source": [
        "###10 hidden layers relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9vQEXDR_IdL"
      },
      "source": [
        "#Creat a net\n",
        "net10_relu = ANN(image_size * image_size, learning_rate= 0.1, max_iterations= 20, train_size= 60000, test_size= 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4UxFnBk_IdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62d51b71-6476-4d5b-89a5-16ae20eb491a"
      },
      "source": [
        "#Call train \n",
        "import time\n",
        "start = time.time()\n",
        "print(\"training...\")\n",
        "net10_relu.train_multi_class_10_relu(train_imgs,train_labels_one_hot)\n",
        "end = time.time()\n",
        "print(end - start)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training...\n",
            "2918.2672584056854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNIkaWTX_IdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "544cd4b6-6bce-42c4-cdb2-1bcb999c4fb9"
      },
      "source": [
        "# Call test\n",
        "# multi-class accuracy, precision and recall https://parasite.id/blog/2018-12-13-model-evaluation/#:~:text=We%20have%20to%20be%20careful,the%20average%20accuracy%20per%20class.&text=This%20gives%20us%20a%20sense,at%20the%20per%2Dclass%20level.\n",
        "#but accuracy for a multiclass classifier is calculated as the average accuracy per class\n",
        "\n",
        "# multi-class confusion matrix https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
        "net10_relu.test_multi_class_10_relu(test_imgs, test_labels_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predit [9. 9. 9. ... 9. 9. 9.]\n",
            "actual [7. 2. 1. ... 4. 5. 6.]\n",
            "TP: [   0.    0.    0.    0.    0.    0.    0.    0.    0. 1009.]\n",
            "FP: [   0.    0.    0.    0.    0.    0.    0.    0.    0. 8991.]\n",
            "TN: [9020. 8865. 8968. 8990. 9018. 9108. 9042. 8972. 9026.    0.]\n",
            "FN: [ 980. 1135. 1032. 1010.  982.  892.  958. 1028.  974.    0.]\n",
            "TP+FP+TN+FN [10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000. 10000.]\n",
            "accuracy: 0.8201799999999999\n",
            "precision: 0.010090000000000002\n",
            "recall: 0.1\n",
            "F1 score: 0.0183304568989009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0nWqZCE4AgH"
      },
      "source": [
        "#Answer Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9NO5tbgEhqx"
      },
      "source": [
        "##Comments for Q2.4 and Q2.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FunPh6KaE5Ln"
      },
      "source": [
        "###Q2.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb5DhCOKGeVk"
      },
      "source": [
        " **How much better are the results for digit recognition, compared to the single-layer perceptron?** \\\n",
        " \\\n",
        "The best result for digit recognition is 99%+ using 1-hidden-layer neural network, wheres the best in single-layer perceptron is ~97%.\n",
        "\n",
        "The best result is saved under the \"change weight initialisation\" in \"implementation of the main method\" section. \\\n",
        " \\\n",
        " \\\n",
        "\n",
        " **How did you modify the initial weights, learning rate, and iterations to achieve this?** \\\n",
        " \\\n",
        "The initial weights is modified using the He initialization. He initialization is variation of random initialisation. Details of the formula can be found in resource section.\n",
        "\n",
        "Changing learning rate from 0.1 to 0.01 also helps increase the accuracy slightly, although it's not as obvious as changing initial weights. Decreasing learning rate further more to 0.001 does not help improve accuracy for sigmoid activation function.\n",
        " \\\n",
        " \\\n",
        "\n",
        " **How much faster/slower is the training time, compared to the single-layer perceptron?** \\\n",
        " \\\n",
        "The training time is ~4min for 1-hidden-layer neural network, whereas for single-layer perceptron it takes ~1min or less. \n",
        "\n",
        "The training time for neural network is also affected by number of hidden layers, number of iterations, and learning rate. The more hidden layer, the longer the training time, the more iterations, the longer the training time, and the smaller the learning rate, the longer the training time.\n",
        " \\\n",
        " \\\n",
        "\n",
        " **How much quicker/slower does the learning converge, compared to the single-layer perceptron?** \\\n",
        " \\\n",
        "20/50 iteration is enough for 1-hidden-layer neural network to converge, increasing the number of iteration to 200 the accuracy starts to go down. Single-layer perceptron does not re-shuffle the data train multiple times, and only took 1 iteration to converge.\n",
        " \\\n",
        " \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCxDk3wRE_pI"
      },
      "source": [
        "###Q2.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLA7RFhWvpii"
      },
      "source": [
        " **How much better are the results for digit recognition, compared to the sigmoid activation function?**\\\n",
        " \\\n",
        "It's about the same accuracy of 99%, and recall and precision ~95%. The best result for 1-hidden-layer neural network using relu can be checked under \"learning rate, activation function, weight initialisation and max iteration\" in \"implemention of the main method\" section. \n",
        "\n",
        "\n",
        " \\\n",
        " \\\n",
        "\n",
        " **How much quicker/slower does the learning converge, compared to the sigmoid activation function?** \\\n",
        " \\\n",
        "Genrally for every number of hidden layer, using sigmoid function takes less time to train compare to using relu function. The learning converge slower compare to using the sigmoid activation function. If use the same learning rate and iterations setting on the same amount of data and just change the activation function from sigmoid to relu, the result wouldn't converge over a large amount of data, and it would end up just predicting 0s.\n",
        "\n",
        "The results included using relu for testing 2-10 hidden layers are still the incorrect one using the learning rate of 0.1 for relu and the network ended up predictiing all 0s for the testing set. Although this still gives an accuracy of 80%, the low recall and precision indicates that these were not a good model overall. These can also be tuned using the same technique for the 1-hidden-layer one as shown.\n",
        "\n",
        " \\\n",
        " \\\n",
        "\n",
        " **How did you modify the initial weights, learning rate, and iterations to achieve this?** \\\n",
        " \\\n",
        "Same as using sigmoid function, the initial weights was changed to being initialised using He initialisation. The learning rate needs to be much smaller to increase the accuracy / enable the network to converge, in this particular case it is changed to 0.0001. The number of iteration is also increase from 20 to 50.\n",
        " \\\n",
        " \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haMJMSSZ0dwn"
      },
      "source": [
        "##Other Reflection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfDFb0lP0kI9"
      },
      "source": [
        "\n",
        "The code can be improved by changing the storage of each hidden layer weights and bias, and somehow implement them through for loops.  Weights initialisation and also be implemented to enable manual change.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7OOYsGjpyqF"
      },
      "source": [
        "##Resource\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH0CxV70qEWp"
      },
      "source": [
        "CS802 Course material\n",
        "\n",
        "[Neural Network in Python using MNIST](https://www.python-course.eu/neural_network_mnist.php)\n",
        "\n",
        "\n",
        "[Multi-Class Confusion Matrix](https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/\n",
        ")\n",
        "\n",
        "\n",
        "[Confusion matrix implementation in Python](https://stackoverflow.com/questions/2148543/how-to-write-a-confusion-matrix-in-python/29877565)\n",
        "\n",
        "\n",
        "[Weight Initialization: He Initialization](https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78)"
      ]
    }
  ]
}